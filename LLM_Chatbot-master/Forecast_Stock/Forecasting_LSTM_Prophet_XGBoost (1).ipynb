{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66003583-b622-42d2-9e57-663f2720d597",
   "metadata": {},
   "source": [
    "LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0672d83-89e4-4e3e-ad68-9f6c4370796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (3.1.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (5.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.4.1+cu124)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.24.6)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from pinecone-client) (2024.6.2)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from pinecone-client) (1.0.3)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from pinecone-client) (4.11.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from pinecone-client) (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system('pip install sentence-transformers pandas pinecone-client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc954571-8f08-4e74-8d69-aa56fb0ac7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: yfinance in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (0.2.43)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (2.32.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (5.2.1)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (2.4.2)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (3.17.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: six>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system('pip install yfinance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5629ac4a-14b2-47e1-90f0-67e611606ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pham ty\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system('pip install tf-keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2456f7fd-c2e2-4957-bb3b-0f9bf69435ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pham Ty\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pham Ty\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "import logging\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2258da-8d8b-47f0-a8fc-bc80e0cea6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['C:\\\\Users\\\\Pham Ty\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "C:\\Users\\Pham Ty\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:__main__:SentenceTransformer 'all-MiniLM-L6-v2' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"09bb980b-6cef-48c3-9aa5-63f3cbc9885e\")\n",
    "index = pc.Index(\"financial-data-index\")\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "logger.info(\"SentenceTransformer 'all-MiniLM-L6-v2' loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c34a8a30-162a-40bd-9d52-3272e8fa111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the CSV files\n",
    "folder_path = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2'\n",
    "\n",
    "# Folder to save the processed CSV files\n",
    "output_folder = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecbbdec5-f0d6-4e7a-9a35-bc6e6097a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục nếu không tồn tại\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8656b539-c169-482a-80e2-128cd7b3da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa danh sách các mã chứng khoán\n",
    "symbols = ['NVDA', 'INTC', 'PLTR', 'TSLA', 'AAPL', 'BBD', 'T', 'SOFI',\n",
    "    'WBD', 'SNAP', 'NIO', 'BTG', 'F', 'AAL', 'NOK', 'BAC',\n",
    "    'CCL', 'ORCL', 'AMD', 'PFE', 'KGC', 'MARA', 'SLB', 'NU',\n",
    "    'MPW', 'MU', 'LCID', 'NCLH', 'RIG', 'AMZN', 'ABEV', 'U',\n",
    "    'LUMN', 'AGNC', 'VZ', 'WBA', 'WFC', 'RIVN', 'UPST', 'GRAB',\n",
    "    'CSCO', 'VALE', 'AVGO', 'PBR', 'GOOGL', 'SMMT', 'GOLD',\n",
    "    'CMG', 'BCS', 'UAA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "958722e7-b7ce-47ff-badd-56d1c1bc8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_stock_data(symbol, start_date='2014-09-18', end_date='2024-09-18'):\n",
    "    \"\"\"\n",
    "    Fetch stock data from Yahoo Finance, save to CSV, and upsert data into Pinecone.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch data from Yahoo Finance\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        \n",
    "        # Save data to CSV\n",
    "        csv_file_path = os.path.join(folder_path, f'{symbol}.csv')\n",
    "        stock_data.to_csv(csv_file_path)\n",
    "        logger.info(f\"Data for {symbol} saved to {csv_file_path}\")\n",
    "        \n",
    "        # Load CSV data\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Select necessary columns and drop NaN values\n",
    "        required_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            logger.error(f\"{csv_file_path} is missing one or more required columns: {required_columns}\")\n",
    "            return\n",
    "        \n",
    "        df = df[required_columns].dropna()\n",
    "\n",
    "        # Convert each row into a single string of concatenated data\n",
    "        text_data = df.apply(\n",
    "            lambda row: f\"Date: {row['Date']}, Open: {row['Open']}, High: {row['High']}, Low: {row['Low']}, Close: {row['Close']}, Adj Close: {row['Adj Close']}, Volume: {row['Volume']}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Generate embeddings for the stock data\n",
    "        embeddings = embedding_model.encode(text_data.tolist(), convert_to_numpy=True)\n",
    "        \n",
    "        # Prepare data for Pinecone upsert\n",
    "        vectors = [(f\"{symbol}_{i}\", embeddings[i].tolist(), {'symbol': symbol}) for i in range(len(embeddings))]\n",
    "        \n",
    "        # Upsert data into Pinecone index\n",
    "        try:\n",
    "            response = index.upsert(vectors)\n",
    "            logger.info(f\"Successfully upserted {len(vectors)} vectors for {symbol}.\")\n",
    "            logger.info(f\"Pinecone response: {response}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error upserting data into Pinecone for {symbol}: {e}\")\n",
    "        \n",
    "        # Save the processed DataFrame to a new CSV file in the output folder\n",
    "        processed_file_path = os.path.join(output_folder, f'processed_{symbol}.csv')\n",
    "        df.to_csv(processed_file_path, index=False)\n",
    "        logger.info(f\"Processed data saved to {processed_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data for {symbol}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a7f5301-8579-437a-adad-082bd0d25526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for NVDA saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\NVDA.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa2eae7197e44388a779e5f1aa5fbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pham Ty\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "ERROR:__main__:Error upserting data into Pinecone for NVDA: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:07:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '3091', 'x-pinecone-request-id': '8024494975870077241', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_NVDA.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for INTC saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\INTC.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b336a940cb441469e3554e2d19e3014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for INTC: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:07:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1633', 'x-pinecone-request-id': '3211537696267647456', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_INTC.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for PLTR saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\PLTR.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd0b00016e148d095bf400aa0ed946c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for PLTR: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:07:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '103', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '594', 'x-pinecone-request-id': '7978418873683502809', 'x-envoy-upstream-service-time': '43', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 384 does not match the dimension of the index 4096\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_PLTR.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for TSLA saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\TSLA.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37af5e5a4c3542bc97af1c957eb2ab64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for TSLA: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:07:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '2314', 'x-pinecone-request-id': '4347086521220707655', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_TSLA.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for AAPL saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\AAPL.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3d6edfcd9548c3a19d0eec5651d953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for AAPL: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:07:42 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1325', 'x-pinecone-request-id': '1725561667838189783', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_AAPL.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for BBD saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\BBD.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fe2400e28d4bc89d0f9c4aaba52b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for BBD: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:07:48 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1275', 'x-pinecone-request-id': '5573807034049010303', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_BBD.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for T saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\T.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e593bfdc87b243e594e1201e66881a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for T: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:07:54 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1579', 'x-pinecone-request-id': '714639371110745926', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4900058 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_T.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for SOFI saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\SOFI.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4157c438a22d4d5ab62fd80fb7c9606f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for SOFI: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:07:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '103', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '587', 'x-pinecone-request-id': '2086710452128365343', 'x-envoy-upstream-service-time': '13', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 384 does not match the dimension of the index 4096\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_SOFI.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for WBD saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\WBD.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f295ecf16242a2ba1c193368c2bae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for WBD: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:03 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1332', 'x-pinecone-request-id': '1711874145403469233', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_WBD.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for SNAP saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\SNAP.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b16168490b49aca768a148bfe3b838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for SNAP: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:08 GMT', 'Content-Type': 'application/json', 'Content-Length': '94', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1037', 'x-pinecone-request-id': '7748428900514211984', 'x-envoy-upstream-service-time': '23', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Request size 3MB exceeds the maximum supported size of 2MB\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_SNAP.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for NIO saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\NIO.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe52b4c809dc4002a977696bd4056379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for NIO: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:12 GMT', 'Content-Type': 'application/json', 'Content-Length': '113', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '852', 'x-pinecone-request-id': '1396744229848004089', 'x-envoy-upstream-service-time': '18', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Number of provided vectors: 1513 exceeds the maximum amount per request: 1000\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_NIO.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for BTG saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\BTG.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4b10c3d7cd4ee29124f7895a74c903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for BTG: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1789', 'x-pinecone-request-id': '2888675448105983651', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_BTG.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for F saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\F.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac6953a35494fbb86c6e2f444fc070f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for F: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:24 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1598', 'x-pinecone-request-id': '7203391156333389013', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4900058 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_F.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for AAL saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\AAL.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938c4b0e78f749edbb58692b59bac384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for AAL: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:31 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1529', 'x-pinecone-request-id': '3051635727439483433', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_AAL.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for NOK saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\NOK.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db58145f601475ba0822fa8755309b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for NOK: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:37 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1297', 'x-pinecone-request-id': '7567449218885963749', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_NOK.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for BAC saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\BAC.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc9a04f341a4085a31aa56b0561fa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for BAC: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1576', 'x-pinecone-request-id': '6008318480974535687', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_BAC.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for CCL saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\CCL.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e629c267faa84eb8baffa1c6b173862d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for CCL: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1581', 'x-pinecone-request-id': '3073999812118748550', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_CCL.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for ORCL saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\ORCL.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07713417b49b4c5b9fc062bc5ed0db41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for ORCL: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:08:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1822', 'x-pinecone-request-id': '2221318875744004556', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_ORCL.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for AMD saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\AMD.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953a9664ede4494b81225df129ad8285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for AMD: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:02 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1885', 'x-pinecone-request-id': '1964591314963815566', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_AMD.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for PFE saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\PFE.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb2738086434c0c8fe4204e245d63ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for PFE: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:09 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1803', 'x-pinecone-request-id': '2322723685997997297', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_PFE.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for KGC saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\KGC.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2ed3114c53464e9b685216c8ac52bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for KGC: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:15 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1553', 'x-pinecone-request-id': '1608427774835720026', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_KGC.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for MARA saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\MARA.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b8794c697d4629954ec43cc0c97532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for MARA: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:21 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1322', 'x-pinecone-request-id': '5452456134057586058', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_MARA.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for SLB saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\SLB.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb6bf67c397425398a78f5c6a50590d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for SLB: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:27 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1281', 'x-pinecone-request-id': '181652461690048002', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_SLB.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for NU saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\NU.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4b0282dbb84f65948a7866b175396d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for NU: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '103', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '318', 'x-pinecone-request-id': '4571459432167715219', 'x-envoy-upstream-service-time': '14', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 384 does not match the dimension of the index 4096\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_NU.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for MPW saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\MPW.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a4554aadd2402cbe0d8a21b48829eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for MPW: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1612', 'x-pinecone-request-id': '1515523576989201188', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_MPW.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for MU saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\MU.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcad812af3b4842859226b8da82a34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for MU: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:42 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1625', 'x-pinecone-request-id': '3352419734925816658', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4905090 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_MU.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for LCID saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\LCID.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474dda784805440d91e56f860c8ceceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for LCID: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:45 GMT', 'Content-Type': 'application/json', 'Content-Length': '113', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '536', 'x-pinecone-request-id': '6902596120182986323', 'x-envoy-upstream-service-time': '12', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Number of provided vectors: 1005 exceeds the maximum amount per request: 1000\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_LCID.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for NCLH saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\NCLH.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ee000b1f004508ab92587528bcff87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for NCLH: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:51 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1298', 'x-pinecone-request-id': '868664592004943425', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_NCLH.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for RIG saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\RIG.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955fd683b1b048b08a573bd56a363ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for RIG: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:09:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1295', 'x-pinecone-request-id': '7263328712559552479', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_RIG.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for AMZN saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\AMZN.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1a72ed68d746c5b4deb5be008cea2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for AMZN: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:03 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1577', 'x-pinecone-request-id': '5351861543962088598', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_AMZN.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for ABEV saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\ABEV.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8b0d89d3c747e691a99a056cf75cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for ABEV: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:09 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1558', 'x-pinecone-request-id': '4849061758501405571', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_ABEV.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for U saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\U.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9f50cb743b4762a6603b7edc601b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for U: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:12 GMT', 'Content-Type': 'application/json', 'Content-Length': '113', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '533', 'x-pinecone-request-id': '2478439018453468748', 'x-envoy-upstream-service-time': '11', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Number of provided vectors: 1005 exceeds the maximum amount per request: 1000\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_U.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for LUMN saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\LUMN.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb28a0f1ab3641438c5609ca58b7371a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for LUMN: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:19 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '2148', 'x-pinecone-request-id': '658396032702718930', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_LUMN.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for AGNC saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\AGNC.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34609700bf654e1384986c512a45197e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for AGNC: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1841', 'x-pinecone-request-id': '1509230651193430211', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_AGNC.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for VZ saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\VZ.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297fc70a67704149af123a8f464b6aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for VZ: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:32 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1620', 'x-pinecone-request-id': '8954527576540799549', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4905090 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_VZ.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for WBA saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\WBA.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71be29d2b75445ea810f72f9fe8fb667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for WBA: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:39 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1535', 'x-pinecone-request-id': '8430160305174858610', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_WBA.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for WFC saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\WFC.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9148eaee3c4934b4fb99269e381c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for WFC: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:45 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1331', 'x-pinecone-request-id': '2757604669065562613', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_WFC.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for RIVN saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\RIVN.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ca44438c9b42a097ca2dc46a8b6bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for RIVN: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:47 GMT', 'Content-Type': 'application/json', 'Content-Length': '103', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '365', 'x-pinecone-request-id': '304263726877646058', 'x-envoy-upstream-service-time': '40', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 384 does not match the dimension of the index 4096\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_RIVN.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for UPST saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\UPST.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08491c9115cc44ed8ae081817cde1eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for UPST: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:50 GMT', 'Content-Type': 'application/json', 'Content-Length': '103', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '567', 'x-pinecone-request-id': '2601456893009162588', 'x-envoy-upstream-service-time': '12', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 384 does not match the dimension of the index 4096\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_UPST.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for GRAB saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\GRAB.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c792f5ce5944a3b32a7eaa072cb8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for GRAB: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:53 GMT', 'Content-Type': 'application/json', 'Content-Length': '103', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '636', 'x-pinecone-request-id': '2982473767928376134', 'x-envoy-upstream-service-time': '23', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 384 does not match the dimension of the index 4096\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_GRAB.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for CSCO saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\CSCO.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211916d58643451cb0754ece74990895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for CSCO: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:10:59 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1380', 'x-pinecone-request-id': '6854453449522764888', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_CSCO.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for VALE saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\VALE.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62afaf483faa4e60946b09505f702d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for VALE: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1407', 'x-pinecone-request-id': '729048944656078346', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_VALE.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for AVGO saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\AVGO.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69f73d4cee443578193ad59d50245a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for AVGO: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:11 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1438', 'x-pinecone-request-id': '2329479377449773491', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_AVGO.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for PBR saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\PBR.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fb39a0c3fb44dd82978c4ca5c5a119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for PBR: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1765', 'x-pinecone-request-id': '199299550043349160', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_PBR.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for GOOGL saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\GOOGL.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b751e574d8784c0ba83c83c8cd926ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for GOOGL: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1363', 'x-pinecone-request-id': '3606640482357862907', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4920186 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_GOOGL.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for SMMT saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\SMMT.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a703ec9b57b44ef98de75b65341dd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for SMMT: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:30 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1352', 'x-pinecone-request-id': '424725694395995345', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4690444 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_SMMT.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for GOLD saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\GOLD.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928dfd47054d496d9ce9a0f98dfc3121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for GOLD: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1813', 'x-pinecone-request-id': '8226543976465180652', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4915154 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_GOLD.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for CMG saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\CMG.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d837e792174734adc4d99b35bffb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for CMG: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1582', 'x-pinecone-request-id': '6272663966791698130', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_CMG.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for BCS saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\BCS.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf871ca43364516a35b5e59f00be57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for BCS: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1708', 'x-pinecone-request-id': '8482135694404558734', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_BCS.csv\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "INFO:__main__:Data for UAA saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2\\UAA.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3013f798ce0b4d2ebbfe8ec4b00f82b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error upserting data into Pinecone for UAA: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 23 Sep 2024 15:11:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '118', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1326', 'x-pinecone-request-id': '5318532030436758107', 'x-envoy-upstream-service-time': '12', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":11,\"message\":\"Error, message length too large: found 4910122 bytes, the limit is: 4194304 bytes\",\"details\":[]}\n",
      "\n",
      "INFO:__main__:Processed data saved to C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\processed_UAA.csv\n"
     ]
    }
   ],
   "source": [
    "# Loop through all symbols and process each\n",
    "for symbol in symbols:\n",
    "    fetch_and_save_stock_data(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a578722-e3fc-4f94-bf6a-c88784f87e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã có đủ dữ liệu cho tất cả 50 mã cổ phiếu.\n"
     ]
    }
   ],
   "source": [
    "csv_files = ['NVDA.csv', 'INTC.csv', 'PLTR.csv', 'TSLA.csv', 'AAPL.csv', 'BBD.csv', 'T.csv', 'SOFI.csv',\n",
    "             'WBD.csv', 'SNAP.csv', 'NIO.csv', 'BTG.csv', 'F.csv', 'AAL.csv', 'NOK.csv', 'BAC.csv', \n",
    "             'CCL.csv', 'ORCL.csv', 'AMD.csv', 'PFE.csv', 'KGC.csv', 'MARA.csv', 'SLB.csv', 'NU.csv', \n",
    "             'MPW.csv', 'MU.csv', 'LCID.csv', 'NCLH.csv', 'RIG.csv', 'AMZN.csv', 'ABEV.csv', 'U.csv', \n",
    "             'LUMN.csv', 'AGNC.csv', 'VZ.csv', 'WBA.csv', 'WFC.csv', 'RIVN.csv', 'UPST.csv', 'GRAB.csv', \n",
    "             'CSCO.csv', 'VALE.csv', 'AVGO.csv', 'PBR.csv', 'GOOGL.csv', 'SMMT.csv', 'GOLD.csv', \n",
    "             'CMG.csv', 'BCS.csv', 'UAA.csv']\n",
    "\n",
    "# Kiểm tra các tệp có trong thư mục\n",
    "csv_files_in_folder = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Lấy các mã từ tên các file CSV\n",
    "csv_symbols_in_folder = [f.split('.')[0] for f in csv_files_in_folder]\n",
    "\n",
    "# Kiểm tra các mã cổ phiếu nào thiếu\n",
    "missing_symbols = [symbol for symbol in symbols if symbol not in csv_symbols_in_folder]\n",
    "\n",
    "# In ra kết quả\n",
    "if missing_symbols:\n",
    "    print(f\"Các mã cổ phiếu bị thiếu: {missing_symbols}\")\n",
    "else:\n",
    "    print(\"Đã có đủ dữ liệu cho tất cả 50 mã cổ phiếu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f962dd38-633c-4ecd-a788-90948b3e4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2abbf9-60c4-4692-9e63-a8c7c01ab2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load stock data from CSV\n",
    "def load_stock_data(symbol):\n",
    "    file_path = os.path.join(folder_path, f'{symbol}.csv')\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "    df = df[['Date', 'Close']]\n",
    "    df = df.sort_values('Date')  # Sắp xếp theo ngày tăng dần\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "290ddaa3-07c4-4bd7-8ab3-36d19bc0648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for LSTM model\n",
    "def prepare_data(df, time_step=60):\n",
    "    data = df['Close'].values\n",
    "    data = data.reshape(-1, 1)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    # Prepare sequences\n",
    "    x_data, y_data = [], []\n",
    "    for i in range(time_step, len(scaled_data)):\n",
    "        x_data.append(scaled_data[i - time_step:i, 0])\n",
    "        y_data.append(scaled_data[i, 0])\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x_data, y_data = np.array(x_data), np.array(y_data)\n",
    "\n",
    "    # Reshape for LSTM [samples, time_steps, features]\n",
    "    x_data = np.reshape(x_data, (x_data.shape[0], x_data.shape[1], 1))\n",
    "\n",
    "    return x_data, y_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc590805-e509-4fc3-a577-c324516fa451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build LSTM model\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(input_shape[1], 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=25))\n",
    "    model.add(Dense(units=1))  # Output layer\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a201412-e3eb-47ac-b561-614c8a1bf62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tất cả kết quả đã được lưu trong C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\forecast_summary.csv\n",
      "\n",
      "Bảng tổng hợp kết quả dự báo:\n",
      "   Symbol       RMSE         MSE      MAPE  \\\n",
      "0    NVDA   7.265612   52.789114  0.072978   \n",
      "1    INTC   2.037902    4.153046  0.046167   \n",
      "2    PLTR   2.022444    4.090278  0.052500   \n",
      "3    TSLA  15.526008  241.056921  0.060866   \n",
      "4    AAPL   8.936813   79.866622  0.040319   \n",
      "5     BBD   0.148475    0.022045  0.037604   \n",
      "6       T   0.635786    0.404223  0.027930   \n",
      "7    SOFI   0.495193    0.245216  0.051159   \n",
      "8     WBD   0.860466    0.740403  0.062352   \n",
      "9    SNAP   1.245678    1.551714  0.075383   \n",
      "10    NIO   1.036498    1.074328  0.125148   \n",
      "11    BTG   0.186767    0.034882  0.044442   \n",
      "12      F   0.652790    0.426134  0.039994   \n",
      "13    AAL   0.732899    0.537140  0.041010   \n",
      "14    NOK   0.142649    0.020349  0.026693   \n",
      "15    BAC   1.261604    1.591644  0.029096   \n",
      "16    CCL   1.041948    1.085655  0.062258   \n",
      "17   ORCL   5.002069   25.020698  0.031134   \n",
      "18    AMD   9.773257   95.516547  0.058236   \n",
      "19    PFE   1.080143    1.166710  0.024636   \n",
      "20    KGC   0.378376    0.143168  0.052046   \n",
      "21   MARA   2.130485    4.538965  0.110598   \n",
      "22    SLB   2.410451    5.810272  0.037553   \n",
      "23     NU   0.714070    0.509897  0.044317   \n",
      "24    MPW   1.235447    1.526330  0.201079   \n",
      "25     MU   5.363034   28.762134  0.043441   \n",
      "26   LCID   0.552843    0.305635  0.160753   \n",
      "27   NCLH   1.325671    1.757403  0.066309   \n",
      "28    RIG   0.487893    0.238040  0.066720   \n",
      "29   AMZN   6.163018   37.982793  0.035969   \n",
      "30   ABEV   0.087026    0.007574  0.026006   \n",
      "31      U   3.085818    9.522275  0.122474   \n",
      "32   LUMN   0.983519    0.967309  0.495432   \n",
      "33   AGNC   0.595642    0.354789  0.052574   \n",
      "34     VZ   1.741532    3.032934  0.039461   \n",
      "35    WBA   3.054660    9.330945  0.144583   \n",
      "36    WFC   2.022214    4.089351  0.034587   \n",
      "37   RIVN   1.169095    1.366782  0.072832   \n",
      "38   UPST   3.419866   11.695487  0.085277   \n",
      "39   GRAB   0.134291    0.018034  0.033687   \n",
      "40   CSCO   1.281728    1.642827  0.020270   \n",
      "41   VALE   0.591844    0.350280  0.032758   \n",
      "42   AVGO   6.929471   48.017573  0.040475   \n",
      "43    PBR   0.599325    0.359190  0.035961   \n",
      "44  GOOGL  10.311046  106.317661  0.066518   \n",
      "45   SMMT   1.409201    1.985847  0.100699   \n",
      "46   GOLD   0.779528    0.607664  0.036328   \n",
      "47    CMG   1.725410    2.977038  0.030363   \n",
      "48    BCS   0.377132    0.142229  0.034652   \n",
      "49    UAA   0.510976    0.261096  0.051254   \n",
      "\n",
      "                                     Predicted_Prices  \\\n",
      "0   12.353496551513672, 12.283124923706055, 12.266...   \n",
      "1   27.758220672607422, 27.50516700744629, 27.3482...   \n",
      "2   17.914701461791992, 17.851360321044922, 17.807...   \n",
      "3   279.5430603027344, 274.9107971191406, 270.0686...   \n",
      "4   148.45034790039062, 147.277587890625, 146.3353...   \n",
      "5   3.7418017387390137, 3.751636505126953, 3.77694...   \n",
      "6   16.179452896118164, 16.096712112426758, 16.041...   \n",
      "7   9.121846199035645, 8.965744972229004, 8.807932...   \n",
      "8   11.418781280517578, 11.334968566894531, 11.317...   \n",
      "9   11.442046165466309, 11.451647758483887, 11.464...   \n",
      "10  11.383442878723145, 11.4608793258667, 11.58946...   \n",
      "11  2.990431308746338, 3.0033085346221924, 3.03335...   \n",
      "12  12.621545791625977, 12.359370231628418, 12.185...   \n",
      "13  12.834623336791992, 12.68867301940918, 12.6041...   \n",
      "14  4.338591575622559, 4.315309524536133, 4.317784...   \n",
      "15  32.21174621582031, 31.902599334716797, 31.7221...   \n",
      "16  9.8767728805542, 9.635030746459961, 9.39676189...   \n",
      "17  66.56632232666016, 65.79027557373047, 65.26907...   \n",
      "18  68.53771209716797, 67.57238006591797, 66.91381...   \n",
      "19  44.09567642211914, 43.9810791015625, 43.918476...   \n",
      "20  3.3377645015716553, 3.389329671859741, 3.45759...   \n",
      "21  10.196187019348145, 10.220121383666992, 10.400...   \n",
      "22  36.63539123535156, 36.53287887573242, 36.58258...   \n",
      "23  11.510618209838867, 11.573856353759766, 11.640...   \n",
      "24  12.882791519165039, 12.7041597366333, 12.54695...   \n",
      "25  50.858680725097656, 50.75870132446289, 50.8708...   \n",
      "26  4.9138970375061035, 4.969820976257324, 5.01837...   \n",
      "27  14.884954452514648, 14.574884414672852, 14.328...   \n",
      "28  2.4565861225128174, 2.423114538192749, 2.41886...   \n",
      "29  118.20823669433594, 117.38650512695312, 116.96...   \n",
      "30  2.8847339153289795, 2.890784740447998, 2.90599...   \n",
      "31  34.03871154785156, 34.78596496582031, 35.59372...   \n",
      "32  8.616548538208008, 8.49985408782959, 8.4239063...   \n",
      "33  10.528623580932617, 10.285673141479492, 10.085...   \n",
      "34  40.8316764831543, 40.65516662597656, 40.539745...   \n",
      "35  34.579872131347656, 34.38660430908203, 34.2796...   \n",
      "36  41.75126266479492, 41.490684509277344, 41.3553...   \n",
      "37  12.02578353881836, 12.059102058410645, 12.1059...   \n",
      "38  45.39485168457031, 44.0835075378418, 42.566776...   \n",
      "39  3.155780076980591, 3.1703407764434814, 3.18243...   \n",
      "40  41.844051361083984, 41.624820709228516, 41.492...   \n",
      "41  12.964226722717285, 13.007335662841797, 13.118...   \n",
      "42  48.447025299072266, 48.025001525878906, 47.744...   \n",
      "43  12.636528015136719, 12.686344146728516, 12.826...   \n",
      "44  94.8995361328125, 94.43204498291016, 94.230178...   \n",
      "45  1.1513935327529907, 1.1418648958206177, 1.1308...   \n",
      "46  14.694648742675781, 14.757546424865723, 14.864...   \n",
      "47  32.46015167236328, 32.1424560546875, 31.892644...   \n",
      "48  7.157820701599121, 7.026683807373047, 6.933450...   \n",
      "49  8.034193992614746, 7.9096293449401855, 7.81182...   \n",
      "\n",
      "                                        Actual_Prices  \\\n",
      "0   12.51200008392334, 13.166999816894531, 13.2089...   \n",
      "1   26.969999313354492, 27.70000076293945, 27.6399...   \n",
      "2   17.840000152587887, 17.950000762939453, 17.25,...   \n",
      "3   242.3999938964844, 249.44000244140625, 240.809...   \n",
      "4   142.44999694824216, 146.1000061035156, 146.399...   \n",
      "5   4.059999942779541, 4.090000152587891, 4.119999...   \n",
      "6   15.899999618530272, 16.09000015258789, 15.9300...   \n",
      "7   8.529999732971191, 8.319999694824219, 8.479999...   \n",
      "8   11.949999809265137, 12.359999656677246, 12.220...   \n",
      "9   11.210000038146973, 11.239999771118164, 11.000...   \n",
      "10  11.729999542236328, 11.90999984741211, 13.1700...   \n",
      "11  3.3800001144409175, 3.4500000476837154, 3.4500...   \n",
      "12  11.470000267028809, 12.359999656677246, 12.510...   \n",
      "13  11.920000076293944, 12.949999809265138, 12.869...   \n",
      "14  4.389999866485596, 4.610000133514404, 4.559999...   \n",
      "15  31.09000015258789, 32.380001068115234, 31.9200...   \n",
      "16  6.849999904632568, 7.760000228881836, 7.429999...   \n",
      "17  63.08000183105469, 65.69000244140625, 66.63999...   \n",
      "18  66.11000061035156, 67.9000015258789, 67.940002...   \n",
      "19  44.13999938964844, 44.459999084472656, 44.1199...   \n",
      "20  3.950000047683716, 4.010000228881836, 3.930000...   \n",
      "21  11.079999923706058, 12.880000114440918, 12.840...   \n",
      "22  38.29999923706055, 39.119998931884766, 41.5699...   \n",
      "23  11.609999656677246, 11.789999961853027, 11.850...   \n",
      "24  11.550000190734863, 11.989999771118162, 11.399...   \n",
      "25  51.72000122070313, 53.959999084472656, 54.7299...   \n",
      "26  4.769999980926514, 4.730000019073486, 4.719999...   \n",
      "27  11.399999618530272, 13.31999969482422, 13.2200...   \n",
      "28  2.6700000762939453, 2.900000095367432, 3.0, 3....   \n",
      "29  115.87999725341793, 121.0899963378906, 120.949...   \n",
      "30  3.109999895095825, 3.0999999046325684, 3.06999...   \n",
      "31  39.36000061035156, 39.310001373291016, 40.9399...   \n",
      "32  7.7800002098083505, 8.039999961853027, 7.28000...   \n",
      "33  8.520000457763672, 9.109999656677246, 8.5, 8.1...   \n",
      "34  39.15999984741211, 39.810001373291016, 39.4000...   \n",
      "35  32.43000030517578, 33.38999938964844, 33.15999...   \n",
      "36  41.58000183105469, 43.540000915527344, 43.3100...   \n",
      "37  12.6899995803833, 12.369999885559082, 11.71000...   \n",
      "38  33.95000076293945, 33.2400016784668, 35.540000...   \n",
      "39  3.299999952316284, 3.2850000858306885, 3.25, 3...   \n",
      "40  41.290000915527344, 41.81999969482421, 41.9900...   \n",
      "41  14.25, 14.640000343322756, 14.800000190734863,...   \n",
      "42  45.678001403808594, 47.98600006103516, 48.4799...   \n",
      "43  13.899999618530272, 13.670000076293944, 14.109...   \n",
      "44  98.63999938964844, 101.63999938964844, 101.430...   \n",
      "45  1.0399999618530271, 1.0299999713897705, 1.0499...   \n",
      "46  15.920000076293944, 16.049999237060547, 15.899...   \n",
      "47  30.0939998626709, 30.722999572753903, 30.39559...   \n",
      "48  6.610000133514404, 6.960000038146973, 6.780000...   \n",
      "49  7.030000209808349, 7.400000095367432, 7.480000...   \n",
      "\n",
      "                             Future_Price_Predictions  \\\n",
      "0   102.10125, 101.81575, 101.118515, 100.1437, 98...   \n",
      "1   19.712574, 19.738972, 19.763554, 19.786413, 19...   \n",
      "2   32.171673, 32.424606, 32.532745, 32.54118, 32....   \n",
      "3   222.41481, 223.35556, 223.95255, 224.32288, 22...   \n",
      "4   210.40439, 209.53821, 208.44276, 207.20317, 20...   \n",
      "5   2.8405488, 2.8374844, 2.8369658, 2.8382492, 2....   \n",
      "6   20.57589, 20.669043, 20.713137, 20.723522, 20....   \n",
      "7   7.176276, 7.1908903, 7.185438, 7.1672034, 7.14...   \n",
      "8   6.8794317, 6.9073734, 6.880416, 6.817997, 6.73...   \n",
      "9   9.344697, 9.403179, 9.460003, 9.515718, 9.5706...   \n",
      "10  5.578654, 5.699878, 5.811927, 5.9182887, 6.021...   \n",
      "11  2.764635, 2.7948697, 2.8082151, 2.8110733, 2.8...   \n",
      "12  10.877166, 10.867076, 10.869254, 10.879747, 10...   \n",
      "13  10.8389225, 10.858101, 10.87381, 10.8872595, 1...   \n",
      "14  4.230653, 4.2210455, 4.213433, 4.207051, 4.201...   \n",
      "15  39.562504, 39.483772, 39.43271, 39.400593, 39....   \n",
      "16  16.217249, 16.282461, 16.319376, 16.335495, 16...   \n",
      "17  147.04941, 148.62132, 149.58235, 150.1072, 150...   \n",
      "18  138.7148, 138.9215, 138.6733, 138.14606, 137.4...   \n",
      "19  29.421797, 29.512348, 29.589666, 29.657743, 29...   \n",
      "20  8.743192, 8.795697, 8.803408, 8.78133, 8.73985...   \n",
      "21  14.828085, 14.833041, 14.8045435, 14.753971, 1...   \n",
      "22  41.020287, 40.826115, 40.690308, 40.59341, 40....   \n",
      "23  14.840897, 14.897981, 14.966901, 15.043957, 15...   \n",
      "24  6.177311, 6.288691, 6.40818, 6.5322633, 6.6584...   \n",
      "25  88.32849, 87.92879, 87.60589, 87.335556, 87.10...   \n",
      "26  4.2169914, 4.2495265, 4.2991443, 4.359608, 4.4...   \n",
      "27  18.778341, 18.972715, 19.14097, 19.291058, 19....   \n",
      "28  3.9650912, 3.9402585, 3.9105177, 3.8775728, 3....   \n",
      "29  175.63011, 176.20032, 176.43224, 176.43115, 17...   \n",
      "30  2.332932, 2.332509, 2.332487, 2.3327262, 2.333...   \n",
      "31  19.539335, 19.848745, 20.156982, 20.464159, 20...   \n",
      "32  6.2482862, 6.321464, 6.400952, 6.484161, 6.569...   \n",
      "33  10.649535, 10.679847, 10.71871, 10.7632065, 10...   \n",
      "34  43.33119, 43.546432, 43.721317, 43.86544, 43.9...   \n",
      "35  13.584449, 13.774531, 14.094441, 14.491716, 14...   \n",
      "36  54.943707, 54.646004, 54.399, 54.193867, 54.02...   \n",
      "37  13.528258, 13.515613, 13.507519, 13.502331, 13...   \n",
      "38  37.01423, 36.728485, 36.555775, 36.459362, 36....   \n",
      "39  3.2864134, 3.2965589, 3.2993124, 3.2971127, 3....   \n",
      "40  49.569004, 49.594772, 49.614594, 49.629368, 49...   \n",
      "41  10.228683, 10.236861, 10.241799, 10.244033, 10...   \n",
      "42  149.13899, 149.41275, 149.39552, 149.16301, 14...   \n",
      "43  14.726776, 14.706134, 14.702694, 14.710305, 14...   \n",
      "44  144.39528, 143.71416, 142.77695, 141.67229, 14...   \n",
      "45  21.646442, 22.89, 23.579529, 23.91942, 24.0332...   \n",
      "46  19.61043, 19.642471, 19.64589, 19.630209, 19.6...   \n",
      "47  54.37105, 54.505047, 54.54242, 54.521458, 54.4...   \n",
      "48  11.557782, 11.527098, 11.495307, 11.462612, 11...   \n",
      "49  7.781093, 7.77513, 7.783713, 7.8014503, 7.8248...   \n",
      "\n",
      "                                         Train_Prices  \n",
      "0   0.4907500147819519, 0.48925000429153437, 0.483...  \n",
      "1   36.22999954223633, 35.91999816894531, 35.56000...  \n",
      "2   27.750000000000004, 25.6299991607666, 24.65999...  \n",
      "3   13.800000190734865, 13.602666854858398, 13.187...  \n",
      "4   27.4325008392334, 27.0575008392334, 26.6875, 2...  \n",
      "5   8.836088180541994, 8.443526268005371, 8.264463...  \n",
      "6   24.29002952575684, 24.35800552368164, 24.27492...  \n",
      "7   17.14999961853027, 17.270000457763672, 16.7199...  \n",
      "8   33.380001068115234, 32.869998931884766, 33.189...  \n",
      "9   21.21999931335449, 21.450000762939453, 21.2099...  \n",
      "10  6.989999771118164, 7.03000020980835, 7.0799999...  \n",
      "11  1.7300000190734866, 1.5199999809265137, 1.5499...  \n",
      "12  14.98999977111816, 14.279999732971191, 14.0900...  \n",
      "13  49.970001220703125, 50.84000015258789, 47.9599...  \n",
      "14  7.869999885559081, 7.739999771118164, 7.730000...  \n",
      "15  17.1299991607666, 16.850000381469727, 16.71999...  \n",
      "16  42.95000076293945, 43.18999862670898, 43.11999...  \n",
      "17  39.95000076293945, 41.11000061035156, 40.63000...  \n",
      "18  2.5199999809265137, 2.470000028610229, 2.5, 2....  \n",
      "19  29.36432647705078, 29.2789363861084, 29.098672...  \n",
      "20  2.9600000381469727, 2.700000047683716, 2.59999...  \n",
      "21  138.72000122070312, 144.47999572753906, 149.03...  \n",
      "22  80.00000000000001, 79.90000152587892, 80.19000...  \n",
      "23  7.579999923706054, 7.960000038146973, 7.050000...  \n",
      "24  13.539999961853027, 13.239999771118162, 13.060...  \n",
      "25  34.0, 33.349998474121094, 32.91999816894531, 3...  \n",
      "26  9.979999542236326, 9.970000267028809, 9.944999...  \n",
      "27  43.790000915527344, 43.7400016784668, 43.75999...  \n",
      "28  16.469999313354492, 16.25, 16.780000686645508,...  \n",
      "29  15.366000175476074, 15.303500175476074, 14.753...  \n",
      "30  5.860000133514404, 5.75, 5.679999828338623, 5....  \n",
      "31  146.72000122070312, 144.5800018310547, 146.690...  \n",
      "32  37.58000183105469, 37.77999877929688, 37.88999...  \n",
      "33  22.239999771118164, 21.86000061035156, 21.7800...  \n",
      "34  45.58000183105469, 45.41999816894531, 45.52999...  \n",
      "35  74.5, 73.44000244140625, 72.38999938964844, 71...  \n",
      "36  53.70000076293945, 53.18999862670897, 52.77000...  \n",
      "37  58.36000061035156, 60.95000076293945, 64.52999...  \n",
      "38  60.04999923706055, 60.790000915527344, 115.089...  \n",
      "39  12.890000343322754, 12.699999809265137, 11.380...  \n",
      "40  26.86000061035156, 26.680000305175785, 26.5900...  \n",
      "41  7.190000057220458, 6.860000133514404, 7.039999...  \n",
      "42  9.920000076293944, 9.821000099182127, 9.732999...  \n",
      "43  7.110000133514405, 6.260000228881836, 6.280000...  \n",
      "44  26.075500488281254, 25.791999816894535, 24.908...  \n",
      "45  10.697999954223633, 10.510000228881836, 10.260...  \n",
      "46  11.470000267028809, 10.449999809265137, 10.319...  \n",
      "47  13.153400421142576, 12.97659969329834, 12.6703...  \n",
      "48  14.460000038146973, 14.109999656677248, 14.229...  \n",
      "49  34.415000915527344, 34.27000045776367, 33.2249...  \n"
     ]
    }
   ],
   "source": [
    "def process_symbol(symbol):\n",
    "    print(f\"Processing {symbol}...\")\n",
    "    df = load_stock_data(symbol)\n",
    "    x_data, y_data, scaler = prepare_data(df)\n",
    "\n",
    "    # Chia dữ liệu thành 8 năm train và 2 năm test\n",
    "    train_size = int(len(x_data) * (8 / 10))  # 8 năm trong tổng số 10 năm\n",
    "    x_train, x_test = x_data[:train_size], x_data[train_size:]\n",
    "    y_train, y_test = y_data[:train_size], y_data[train_size:]\n",
    "\n",
    "    # Inverse transform y_train và y_test để có giá trị gốc\n",
    "    y_train_original = scaler.inverse_transform(y_train.reshape(-1, 1)).flatten().tolist()\n",
    "    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten().tolist()\n",
    "\n",
    "    # Xây dựng và huấn luyện mô hình\n",
    "    model = build_lstm_model(x_train.shape)\n",
    "    model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=0)\n",
    "\n",
    "    # Dự đoán giá trên tập test\n",
    "    predicted_prices = model.predict(x_test)\n",
    "    predicted_prices = scaler.inverse_transform(predicted_prices).flatten().tolist()\n",
    "\n",
    "    # Tính toán các chỉ số đánh giá\n",
    "    mse = mean_squared_error(y_test_original, predicted_prices)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test_original, predicted_prices)\n",
    "\n",
    "    # Dự báo giá cho 1 năm tiếp theo (nếu cần)\n",
    "    future_prices = []\n",
    "    last_sequence = x_test[-1]\n",
    "    for _ in range(365):\n",
    "        next_price = model.predict(last_sequence.reshape(1, -1, 1))\n",
    "        future_price = scaler.inverse_transform(next_price)\n",
    "        future_prices.append(future_price[0, 0])\n",
    "        last_sequence = np.append(last_sequence[1:], next_price, axis=0)\n",
    "\n",
    "    # Trả về kết quả, bao gồm cả Train_Prices và Test_Prices\n",
    "    return {\n",
    "        'Symbol': symbol,\n",
    "        'RMSE': rmse,\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'Predicted_Prices': predicted_prices,\n",
    "        'Actual_Prices': y_test_original,\n",
    "        'Future_Price_Predictions': future_prices,\n",
    "        'Train_Prices': y_train_original\n",
    "    }\n",
    "\n",
    "# Main processing block\n",
    "if __name__ == '__main__':\n",
    "    # Create a list to store forecast results\n",
    "    forecast_results = []\n",
    "\n",
    "    # Parallel processing for all symbols\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_symbol)(symbol) for symbol in symbols\n",
    "    )\n",
    "\n",
    "    # Add the results to forecast_results list\n",
    "    forecast_results.extend(results)\n",
    "\n",
    "    # Create the summary DataFrame\n",
    "    summary_df = pd.DataFrame(forecast_results)\n",
    "\n",
    "    # Convert lists to strings for saving\n",
    "    summary_df['Predicted_Prices'] = summary_df['Predicted_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Actual_Prices'] = summary_df['Actual_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Future_Price_Predictions'] = summary_df['Future_Price_Predictions'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Train_Prices'] = summary_df['Train_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "\n",
    "    # Save the summary DataFrame to CSV\n",
    "    summary_file = os.path.join(output_folder, 'forecast_summary.csv')\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "    print(f\"Tất cả kết quả đã được lưu trong {summary_file}\")\n",
    "\n",
    "    # Print the summary DataFrame\n",
    "    print(\"\\nBảng tổng hợp kết quả dự báo:\")\n",
    "    print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db24382d-381d-4ff3-b105-25a9d1c86f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 'forecast_summary.csv' successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a12aa95940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dash\n",
    "from dash import html, dcc\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Paths to data\n",
    "folder_path = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2'  # Folder with stock CSV files\n",
    "output_folder = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2'  # Output folder\n",
    "\n",
    "# List of stock symbols\n",
    "symbols = ['NVDA', 'INTC', 'PLTR', 'TSLA', 'AAPL', 'BBD', 'T', 'SOFI',\n",
    "           'WBD', 'SNAP', 'NIO', 'BTG', 'F', 'AAL', 'NOK', 'BAC',\n",
    "           'CCL', 'ORCL', 'AMD', 'PFE', 'KGC', 'MARA', 'SLB', 'NU',\n",
    "           'MPW', 'MU', 'LCID', 'NCLH', 'RIG', 'AMZN', 'ABEV', 'U',\n",
    "           'LUMN', 'AGNC', 'VZ', 'WBA', 'WFC', 'RIVN', 'UPST', 'GRAB',\n",
    "           'CSCO', 'VALE', 'AVGO', 'PBR', 'GOOGL', 'SMMT', 'GOLD',\n",
    "           'CMG', 'BCS', 'UAA']\n",
    "\n",
    "# Load forecast_summary.csv\n",
    "forecast_summary_file = os.path.join(output_folder, 'forecast_summary.csv')\n",
    "if os.path.exists(forecast_summary_file):\n",
    "    forecast_summary_df = pd.read_csv(forecast_summary_file)\n",
    "    # Parse string lists into actual lists\n",
    "    for col in ['Predicted_Prices', 'Actual_Prices', 'Future_Price_Predictions', 'Train_Prices']:\n",
    "        if col in forecast_summary_df.columns:\n",
    "            forecast_summary_df[col] = forecast_summary_df[col].apply(\n",
    "                lambda x: [float(item.strip()) for item in x.split(',')] if pd.notnull(x) else []\n",
    "            )\n",
    "    logging.info(\"Loaded 'forecast_summary.csv' successfully.\")\n",
    "else:\n",
    "    forecast_summary_df = pd.DataFrame()\n",
    "    logging.warning(\"Could not find 'forecast_summary.csv'. Evaluation metrics and forecasts will not be displayed.\")\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server  # For deployment\n",
    "\n",
    "# Define app layout\n",
    "app.layout = html.Div([\n",
    "    html.H1('Stock Price Predictions Dashboard'),\n",
    "    html.Div([\n",
    "        html.Label('Select Stock Symbol'),\n",
    "        dcc.Dropdown(\n",
    "            id='stock-dropdown',\n",
    "            options=[{'label': s, 'value': s} for s in symbols],\n",
    "            value='NVDA'  # Default stock\n",
    "        )\n",
    "    ], style={'width': '25%', 'display': 'inline-block'}),\n",
    "    html.Div([\n",
    "        html.Label('Select Date Range'),\n",
    "        dcc.DatePickerRange(\n",
    "            id='date-picker',\n",
    "            min_date_allowed='2014-09-18',  # Adjusted min date for 10-year period\n",
    "            max_date_allowed='2024-09-18',  # Adjusted max date for 10-year period\n",
    "            start_date='2014-09-18',        # Default start date\n",
    "            end_date='2024-09-18'           # Default end date\n",
    "        )\n",
    "    ], style={'display': 'inline-block', 'marginLeft': '50px'}),\n",
    "    html.Div([\n",
    "        html.Label('Show 1-Year Forecast'),\n",
    "        dcc.Checklist(\n",
    "            id='forecast-checkbox',\n",
    "            options=[{'label': 'Include 1-Year Forecast', 'value': 'show_forecast'}],\n",
    "            value=[],\n",
    "            inline=True\n",
    "        )\n",
    "    ], style={'marginTop': '20px'}),\n",
    "    dcc.Graph(id='price-graph'),\n",
    "    html.Div(id='metrics-output', style={'marginTop': '20px'})\n",
    "])\n",
    "\n",
    "# Define callback to update graph and evaluation metrics\n",
    "@app.callback(\n",
    "    [Output('price-graph', 'figure'),\n",
    "     Output('metrics-output', 'children')],\n",
    "    [Input('stock-dropdown', 'value'),\n",
    "     Input('date-picker', 'start_date'),\n",
    "     Input('date-picker', 'end_date'),\n",
    "     Input('forecast-checkbox', 'value')]\n",
    ")\n",
    "def update_graph(selected_stock, start_date, end_date, forecast_option):\n",
    "    # Convert start_date and end_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Get the forecast data for the selected stock\n",
    "    forecast_row = forecast_summary_df[forecast_summary_df['Symbol'] == selected_stock]\n",
    "\n",
    "    if forecast_row.empty:\n",
    "        logging.error(f\"No forecast data found for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"No forecast data found for {selected_stock}.\"\n",
    "\n",
    "    # Parse 'Actual_Prices', 'Predicted_Prices', 'Future_Price_Predictions', 'Train_Prices'\n",
    "    actual_prices = forecast_row.iloc[0]['Actual_Prices']\n",
    "    predicted_prices = forecast_row.iloc[0]['Predicted_Prices']\n",
    "    future_prices = forecast_row.iloc[0]['Future_Price_Predictions']\n",
    "    train_prices = forecast_row.iloc[0]['Train_Prices']\n",
    "\n",
    "    # Load the stock data to get dates\n",
    "    stock_data_file = os.path.join(folder_path, f'{selected_stock}.csv')\n",
    "    if not os.path.exists(stock_data_file):\n",
    "        logging.error(f\"Stock data file not found for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Stock data file not found for {selected_stock}.\"\n",
    "\n",
    "    df_stock = pd.read_csv(stock_data_file)\n",
    "    if 'Date' not in df_stock.columns or 'Close' not in df_stock.columns:\n",
    "        logging.error(f\"Incorrect data format in stock data file for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Incorrect data format in stock data file for {selected_stock}.\"\n",
    "\n",
    "    # Convert 'Date' to datetime\n",
    "    df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
    "    dates = df_stock['Date']\n",
    "    data_length = len(df_stock)\n",
    "\n",
    "    time_step = 60  # As used in your model\n",
    "    total_samples = data_length - time_step\n",
    "\n",
    "    # Calculate train_size and test_size based on 10-year data\n",
    "    # Assuming 8 years for training and 2 years for testing\n",
    "    # Calculate the number of samples per year\n",
    "    # For simplicity, assume 252 trading days per year\n",
    "    samples_per_year = 252\n",
    "    train_size = 8 * samples_per_year\n",
    "    test_size = 2 * samples_per_year\n",
    "\n",
    "    # Adjust train_size and test_size if total_samples is less\n",
    "    if train_size + test_size > total_samples:\n",
    "        train_size = int(total_samples * 0.8)\n",
    "        test_size = total_samples - train_size\n",
    "        logging.warning(f\"Adjusted train_size to {train_size} and test_size to {test_size} due to insufficient data.\")\n",
    "\n",
    "    # Ensure that the length of 'Actual_Prices' matches 'test_size'\n",
    "    if len(actual_prices) != test_size or len(predicted_prices) != test_size:\n",
    "        logging.error(f\"Length mismatch between actual/predicted prices and test set for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Length mismatch in data for {selected_stock}.\"\n",
    "\n",
    "    # Get indices for train and test sets in original data\n",
    "    train_indices_in_y_data = range(0, train_size)\n",
    "    test_indices_in_y_data = range(train_size, train_size + test_size)\n",
    "    indices_train = [i + time_step for i in train_indices_in_y_data]\n",
    "    indices_test = [i + time_step for i in test_indices_in_y_data]\n",
    "\n",
    "    # Ensure indices do not exceed the length of dates\n",
    "    indices_train = [i for i in indices_train if i < len(dates)]\n",
    "    indices_test = [i for i in indices_test if i < len(dates)]\n",
    "\n",
    "    # Dates for train and test sets\n",
    "    dates_train = dates.iloc[indices_train].reset_index(drop=True)\n",
    "    dates_test = dates.iloc[indices_test].reset_index(drop=True)\n",
    "\n",
    "    # Create DataFrame for train set\n",
    "    df_train = pd.DataFrame({\n",
    "        'Date': dates_train,\n",
    "        'Train_Price': train_prices\n",
    "    })\n",
    "\n",
    "    # Create DataFrame for test set\n",
    "    df_test = pd.DataFrame({\n",
    "        'Date': dates_test,\n",
    "        'Actual_Price': actual_prices,\n",
    "        'Predicted_Price': predicted_prices\n",
    "    })\n",
    "\n",
    "    # Filter df_train and df_test based on 'start_date' and 'end_date'\n",
    "    df_train_filtered = df_train[(df_train['Date'] >= start_date) & (df_train['Date'] <= end_date)]\n",
    "    df_test_filtered = df_test[(df_test['Date'] >= start_date) & (df_test['Date'] <= end_date)]\n",
    "\n",
    "    # Initialize data list for plotting\n",
    "    data = []\n",
    "\n",
    "    # Add Train Prices if available in the selected date range\n",
    "    if not df_train_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_train_filtered['Date'],\n",
    "                y=df_train_filtered['Train_Price'],\n",
    "                mode='lines',\n",
    "                name='Train Price',\n",
    "                line=dict(color='red')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add Actual Test Prices\n",
    "    if not df_test_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_test_filtered['Date'],\n",
    "                y=df_test_filtered['Actual_Price'],\n",
    "                mode='lines',\n",
    "                name='Actual Test Price',\n",
    "                line=dict(color='green')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add Predicted Test Prices\n",
    "    if not df_test_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_test_filtered['Date'],\n",
    "                y=df_test_filtered['Predicted_Price'],\n",
    "                mode='lines',\n",
    "                name='Predicted Test Price',\n",
    "                line=dict(color='blue')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add future price predictions if selected\n",
    "    if 'show_forecast' in forecast_option:\n",
    "        # Generate future dates\n",
    "        last_date = dates.iloc[-1]\n",
    "        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=len(future_prices), freq='D')\n",
    "\n",
    "        if len(future_prices) != len(future_dates):\n",
    "            logging.error(f\"Length mismatch between future prices and future dates for {selected_stock}.\")\n",
    "        else:\n",
    "            data.append(\n",
    "                go.Scatter(\n",
    "                    x=future_dates,\n",
    "                    y=future_prices,\n",
    "                    mode='lines',\n",
    "                    name='1-Year Forecast',\n",
    "                    line=dict(dash='dash', color='orange')\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Create figure\n",
    "    figure = {\n",
    "        'data': data,\n",
    "        'layout': go.Layout(\n",
    "            title=f'{selected_stock} Price Prediction (2014-09-18 to 2024-09-18)',\n",
    "            xaxis={'title': 'Date'},\n",
    "            yaxis={'title': 'Price'},\n",
    "            hovermode='closest'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Get RMSE, MSE, and MAPE\n",
    "    rmse = forecast_row.iloc[0]['RMSE']\n",
    "    mse = forecast_row.iloc[0]['MSE']\n",
    "    mape = forecast_row.iloc[0]['MAPE']\n",
    "\n",
    "    metrics_output = [\n",
    "        html.P(f'Root Mean Squared Error (RMSE): {rmse:.4f}'),\n",
    "        html.P(f'Mean Squared Error (MSE): {mse:.4f}'),\n",
    "        html.P(f'Mean Absolute Percentage Error (MAPE): {mape:.4%}')\n",
    "    ]\n",
    "\n",
    "    return figure, metrics_output\n",
    "\n",
    "# Run Dash app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c5920d-211b-4abf-847a-938786bda5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 20:52:38,783 INFO:Loaded 'forecast_summary.csv' successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15651471340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 20:52:39,209 WARNING:Adjusted train_size to 1964 and test_size to 492 due to insufficient data.\n",
      "2024-10-21 20:52:41,786 WARNING:Adjusted train_size to 1964 and test_size to 492 due to insufficient data.\n",
      "2024-10-21 20:52:45,226 WARNING:Adjusted train_size to 1964 and test_size to 492 due to insufficient data.\n",
      "2024-10-21 20:52:48,580 WARNING:Adjusted train_size to 1964 and test_size to 492 due to insufficient data.\n",
      "2024-10-21 20:52:50,206 WARNING:Adjusted train_size to 1964 and test_size to 492 due to insufficient data.\n",
      "2024-10-21 20:52:51,982 WARNING:Adjusted train_size to 1964 and test_size to 492 due to insufficient data.\n",
      "2024-10-21 20:52:54,784 WARNING:Adjusted train_size to 1964 and test_size to 492 due to insufficient data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dash\n",
    "from dash import html, dcc\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Paths to data\n",
    "folder_path = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2'  # Folder with stock CSV files\n",
    "output_folder = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2'  # Output folder\n",
    "\n",
    "# List of stock symbols\n",
    "symbols = ['NVDA', 'INTC', 'PLTR', 'TSLA', 'AAPL', 'BBD', 'T', 'SOFI',\n",
    "           'WBD', 'SNAP', 'NIO', 'BTG', 'F', 'AAL', 'NOK', 'BAC',\n",
    "           'CCL', 'ORCL', 'AMD', 'PFE', 'KGC', 'MARA', 'SLB', 'NU',\n",
    "           'MPW', 'MU', 'LCID', 'NCLH', 'RIG', 'AMZN', 'ABEV', 'U',\n",
    "           'LUMN', 'AGNC', 'VZ', 'WBA', 'WFC', 'RIVN', 'UPST', 'GRAB',\n",
    "           'CSCO', 'VALE', 'AVGO', 'PBR', 'GOOGL', 'SMMT', 'GOLD',\n",
    "           'CMG', 'BCS', 'UAA']\n",
    "\n",
    "# Load forecast_summary.csv\n",
    "forecast_summary_file = os.path.join(output_folder, 'forecast_summary.csv')\n",
    "if os.path.exists(forecast_summary_file):\n",
    "    forecast_summary_df = pd.read_csv(forecast_summary_file)\n",
    "    # Parse string lists into actual lists\n",
    "    for col in ['Predicted_Prices', 'Actual_Prices', 'Future_Price_Predictions', 'Train_Prices']:\n",
    "        if col in forecast_summary_df.columns:\n",
    "            forecast_summary_df[col] = forecast_summary_df[col].apply(\n",
    "                lambda x: [float(item.strip()) for item in x.split(',')] if pd.notnull(x) else []\n",
    "            )\n",
    "    logging.info(\"Loaded 'forecast_summary.csv' successfully.\")\n",
    "else:\n",
    "    forecast_summary_df = pd.DataFrame()\n",
    "    logging.warning(\"Could not find 'forecast_summary.csv'. Evaluation metrics and forecasts will not be displayed.\")\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server  # For deployment\n",
    "\n",
    "# Define app layout\n",
    "app.layout = html.Div([\n",
    "    html.H1('Stock Price Predictions Dashboard'),\n",
    "    html.Div([\n",
    "        html.Label('Select Stock Symbol'),\n",
    "        dcc.Dropdown(\n",
    "            id='stock-dropdown',\n",
    "            options=[{'label': s, 'value': s} for s in symbols],\n",
    "            value='NVDA'  # Default stock\n",
    "        )\n",
    "    ], style={'width': '25%', 'display': 'inline-block'}),\n",
    "    html.Div([\n",
    "        html.Label('Select Date Range'),\n",
    "        dcc.DatePickerRange(\n",
    "            id='date-picker',\n",
    "            min_date_allowed='2014-09-18',  # Adjusted min date for 10-year period\n",
    "            max_date_allowed='2024-09-18',  # Adjusted max date for 10-year period\n",
    "            start_date='2014-09-18',        # Default start date\n",
    "            end_date='2024-09-18'           # Default end date\n",
    "        )\n",
    "    ], style={'display': 'inline-block', 'marginLeft': '50px'}),\n",
    "    html.Div([\n",
    "        html.Label('Show 1-Year Forecast'),\n",
    "        dcc.Checklist(\n",
    "            id='forecast-checkbox',\n",
    "            options=[{'label': 'Include 1-Year Forecast', 'value': 'show_forecast'}],\n",
    "            value=[],\n",
    "            inline=True\n",
    "        )\n",
    "    ], style={'marginTop': '20px'}),\n",
    "    dcc.Graph(id='price-graph'),\n",
    "    html.Div(id='metrics-output', style={'marginTop': '20px'})\n",
    "])\n",
    "\n",
    "# Define callback to update graph and evaluation metrics\n",
    "@app.callback(\n",
    "    [Output('price-graph', 'figure'),\n",
    "     Output('metrics-output', 'children')],\n",
    "    [Input('stock-dropdown', 'value'),\n",
    "     Input('date-picker', 'start_date'),\n",
    "     Input('date-picker', 'end_date'),\n",
    "     Input('forecast-checkbox', 'value')]\n",
    ")\n",
    "def update_graph(selected_stock, start_date, end_date, forecast_option):\n",
    "    # Convert start_date and end_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Get the forecast data for the selected stock\n",
    "    forecast_row = forecast_summary_df[forecast_summary_df['Symbol'] == selected_stock]\n",
    "\n",
    "    if forecast_row.empty:\n",
    "        logging.error(f\"No forecast data found for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"No forecast data found for {selected_stock}.\"\n",
    "\n",
    "    # Parse 'Actual_Prices', 'Predicted_Prices', 'Future_Price_Predictions', 'Train_Prices'\n",
    "    actual_prices = forecast_row.iloc[0]['Actual_Prices']\n",
    "    predicted_prices = forecast_row.iloc[0]['Predicted_Prices']\n",
    "    future_prices = forecast_row.iloc[0]['Future_Price_Predictions']\n",
    "    train_prices = forecast_row.iloc[0]['Train_Prices']\n",
    "\n",
    "    # Load the stock data to get dates\n",
    "    stock_data_file = os.path.join(folder_path, f'{selected_stock}.csv')\n",
    "    if not os.path.exists(stock_data_file):\n",
    "        logging.error(f\"Stock data file not found for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Stock data file not found for {selected_stock}.\"\n",
    "\n",
    "    df_stock = pd.read_csv(stock_data_file)\n",
    "    if 'Date' not in df_stock.columns or 'Close' not in df_stock.columns:\n",
    "        logging.error(f\"Incorrect data format in stock data file for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Incorrect data format in stock data file for {selected_stock}.\"\n",
    "\n",
    "    # Convert 'Date' to datetime\n",
    "    df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
    "    dates = df_stock['Date']\n",
    "    data_length = len(df_stock)\n",
    "\n",
    "    time_step = 60  # As used in your model\n",
    "    total_samples = data_length - time_step\n",
    "\n",
    "    # Calculate train_size and test_size based on 10-year data\n",
    "    # Assuming 8 years for training and 2 years for testing\n",
    "    # Calculate the number of samples per year\n",
    "    # For simplicity, assume 252 trading days per year\n",
    "    samples_per_year = 252\n",
    "    train_size = 8 * samples_per_year\n",
    "    test_size = 2 * samples_per_year\n",
    "\n",
    "    # Adjust train_size and test_size if total_samples is less\n",
    "    if train_size + test_size > total_samples:\n",
    "        train_size = int(total_samples * 0.8)\n",
    "        test_size = total_samples - train_size\n",
    "        logging.warning(f\"Adjusted train_size to {train_size} and test_size to {test_size} due to insufficient data.\")\n",
    "\n",
    "    # Ensure that the length of 'Actual_Prices' matches 'test_size'\n",
    "    if len(actual_prices) != test_size or len(predicted_prices) != test_size:\n",
    "        logging.error(f\"Length mismatch between actual/predicted prices and test set for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Length mismatch in data for {selected_stock}.\"\n",
    "\n",
    "    # Get indices for train and test sets in original data\n",
    "    train_indices_in_y_data = range(0, train_size)\n",
    "    test_indices_in_y_data = range(train_size, train_size + test_size)\n",
    "    indices_train = [i + time_step for i in train_indices_in_y_data]\n",
    "    indices_test = [i + time_step for i in test_indices_in_y_data]\n",
    "\n",
    "    # Ensure indices do not exceed the length of dates\n",
    "    indices_train = [i for i in indices_train if i < len(dates)]\n",
    "    indices_test = [i for i in indices_test if i < len(dates)]\n",
    "\n",
    "    # Dates for train and test sets\n",
    "    dates_train = dates.iloc[indices_train].reset_index(drop=True)\n",
    "    dates_test = dates.iloc[indices_test].reset_index(drop=True)\n",
    "\n",
    "    # Create DataFrame for train set\n",
    "    df_train = pd.DataFrame({\n",
    "        'Date': dates_train,\n",
    "        'Train_Price': train_prices\n",
    "    })\n",
    "\n",
    "    # Create DataFrame for test set\n",
    "    df_test = pd.DataFrame({\n",
    "        'Date': dates_test,\n",
    "        'Actual_Price': actual_prices,\n",
    "        'Predicted_Price': predicted_prices\n",
    "    })\n",
    "\n",
    "    # Determine the split date (end of train)\n",
    "    if not df_train.empty:\n",
    "        split_date = df_train['Date'].max()\n",
    "    else:\n",
    "        split_date = None\n",
    "        logging.warning(f\"Train data is empty for {selected_stock}.\")\n",
    "\n",
    "    # Filter df_train and df_test based on 'start_date' and 'end_date'\n",
    "    df_train_filtered = df_train[(df_train['Date'] >= start_date) & (df_train['Date'] <= end_date)]\n",
    "    df_test_filtered = df_test[(df_test['Date'] >= start_date) & (df_test['Date'] <= end_date)]\n",
    "\n",
    "    # Initialize data list for plotting\n",
    "    data = []\n",
    "\n",
    "    # Add Train Prices if available in the selected date range\n",
    "    if not df_train_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_train_filtered['Date'],\n",
    "                y=df_train_filtered['Train_Price'],\n",
    "                mode='lines',\n",
    "                name='Train Price',\n",
    "                line=dict(color='red')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add Actual Test Prices\n",
    "    if not df_test_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_test_filtered['Date'],\n",
    "                y=df_test_filtered['Actual_Price'],\n",
    "                mode='lines',\n",
    "                name='Actual Test Price',\n",
    "                line=dict(color='green')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add Predicted Test Prices\n",
    "    if not df_test_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_test_filtered['Date'],\n",
    "                y=df_test_filtered['Predicted_Price'],\n",
    "                mode='lines',\n",
    "                name='Predicted Test Price',\n",
    "                line=dict(color='blue')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add vertical line to indicate the split between train and test\n",
    "    if split_date:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=[split_date, split_date],\n",
    "                y=[df_stock['Close'].min(), df_stock['Close'].max()],\n",
    "                mode='lines',\n",
    "                name='Train-Test Split',\n",
    "                line=dict(color='black', dash='dash')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add future price predictions if selected\n",
    "    if 'show_forecast' in forecast_option:\n",
    "        # Generate future dates\n",
    "        last_date = dates.iloc[-1]\n",
    "        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=len(future_prices), freq='D')\n",
    "\n",
    "        if len(future_prices) != len(future_dates):\n",
    "            logging.error(f\"Length mismatch between future prices and future dates for {selected_stock}.\")\n",
    "        else:\n",
    "            data.append(\n",
    "                go.Scatter(\n",
    "                    x=future_dates,\n",
    "                    y=future_prices,\n",
    "                    mode='lines',\n",
    "                    name='1-Year Forecast',\n",
    "                    line=dict(dash='dash', color='orange')\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Create figure\n",
    "    figure = {\n",
    "        'data': data,\n",
    "        'layout': go.Layout(\n",
    "            title=f'{selected_stock} Price Prediction (2014-09-18 to 2024-09-18)',\n",
    "            xaxis={'title': 'Date'},\n",
    "            yaxis={'title': 'Price'},\n",
    "            hovermode='closest'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Get RMSE, MSE, and MAPE\n",
    "    rmse = forecast_row.iloc[0]['RMSE']\n",
    "    mse = forecast_row.iloc[0]['MSE']\n",
    "    mape = forecast_row.iloc[0]['MAPE']\n",
    "\n",
    "    metrics_output = [\n",
    "        html.P(f'Root Mean Squared Error (RMSE): {rmse:.4f}'),\n",
    "        html.P(f'Mean Squared Error (MSE): {mse:.4f}'),\n",
    "        html.P(f'Mean Absolute Percentage Error (MAPE): {mape:.4%}')\n",
    "    ]\n",
    "\n",
    "    return figure, metrics_output\n",
    "\n",
    "# Run Dash app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False, use_reloader=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d810ac-37e4-4262-9498-15cedfa8f1cc",
   "metadata": {},
   "source": [
    "PROPHET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fa48e9-b559-4648-a206-632927d98afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting prophet\n",
      "  Downloading prophet-1.1.5-py3-none-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting cmdstanpy>=1.0.4 (from prophet)\n",
      "  Downloading cmdstanpy-1.2.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (3.8.4)\n",
      "Requirement already satisfied: pandas>=1.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (2.2.2)\n",
      "Collecting holidays>=0.25 (from prophet)\n",
      "  Downloading holidays-0.57-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (4.66.4)\n",
      "Collecting importlib-resources (from prophet)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting stanio<2.0.0,>=0.4.0 (from cmdstanpy>=1.0.4->prophet)\n",
      "  Downloading stanio-0.5.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from holidays>=0.25->prophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.4->prophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.4->prophet) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->prophet) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil->holidays>=0.25->prophet) (1.16.0)\n",
      "Downloading prophet-1.1.5-py3-none-win_amd64.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.3 MB 2.0 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.2/13.3 MB 2.8 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.7/13.3 MB 5.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.7/13.3 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.0/13.3 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.5/13.3 MB 20.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.8/13.3 MB 18.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.1/13.3 MB 19.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.7/13.3 MB 19.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.8/13.3 MB 20.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.2/13.3 MB 19.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.9/13.3 MB 24.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.7/13.3 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.5/13.3 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.3/13.3 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 19.9 MB/s eta 0:00:00\n",
      "Downloading cmdstanpy-1.2.4-py3-none-any.whl (94 kB)\n",
      "   ---------------------------------------- 0.0/94.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 94.5/94.5 kB ? eta 0:00:00\n",
      "Downloading holidays-0.57-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 67.0 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading stanio-0.5.1-py3-none-any.whl (8.1 kB)\n",
      "Installing collected packages: stanio, importlib-resources, holidays, cmdstanpy, prophet\n",
      "Successfully installed cmdstanpy-1.2.4 holidays-0.57 importlib-resources-6.4.5 prophet-1.1.5 stanio-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts install_cmdstan.exe and install_cxx_toolchain.exe are installed in 'C:\\Users\\Pham Ty\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528b1c58-a36f-4cba-9da8-e2f5df232e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['C:\\\\Users\\\\Pham Ty\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:SentenceTransformer 'all-MiniLM-L6-v2' loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã có đủ dữ liệu cho tất cả 50 mã cổ phiếu.\n",
      "Tất cả kết quả đã được lưu trong C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\forecast_summary_Prophet.csv\n",
      "\n",
      "Bảng tổng hợp kết quả dự báo:\n",
      "   Symbol        RMSE           MSE      MAPE  \\\n",
      "0    NVDA   43.223425   1868.264492  0.495865   \n",
      "1    INTC   14.109645    199.082075  0.353835   \n",
      "2    PLTR         NaN           NaN       NaN   \n",
      "3    TSLA  167.091559  27919.589035  0.835795   \n",
      "4    AAPL   21.253842    451.725815  0.110963   \n",
      "5     BBD    0.600685      0.360822  0.165139   \n",
      "6       T    3.083694      9.509168  0.139693   \n",
      "7    SOFI         NaN           NaN       NaN   \n",
      "8     WBD   18.259470    333.408238  1.670443   \n",
      "9    SNAP         NaN           NaN       NaN   \n",
      "10    NIO         NaN           NaN       NaN   \n",
      "11    BTG    0.899514      0.809125  0.260761   \n",
      "12      F    9.043384     81.782797  0.726426   \n",
      "13    AAL    2.056905      4.230859  0.115432   \n",
      "14    NOK    2.143053      4.592675  0.525973   \n",
      "15    BAC    9.426773     88.864047  0.275190   \n",
      "16    CCL   12.679328    160.765363  0.703655   \n",
      "17   ORCL   29.652767    879.286603  0.218053   \n",
      "18    AMD   34.769441   1208.913999  0.289963   \n",
      "19    PFE   33.185239   1101.260085  0.975336   \n",
      "20    KGC    5.180175     26.834216  0.709725   \n",
      "21   MARA   13.825670    191.149162  0.895210   \n",
      "22    SLB    9.528300     90.788500  0.163029   \n",
      "23     NU         NaN           NaN       NaN   \n",
      "24    MPW    8.075823     65.218916  1.390765   \n",
      "25     MU   38.650016   1493.823700  0.298668   \n",
      "26   LCID         NaN           NaN       NaN   \n",
      "27   NCLH   11.116469    123.575876  0.537119   \n",
      "28    RIG    2.010379      4.041622  0.250227   \n",
      "29   AMZN   61.780958   3816.886755  0.328943   \n",
      "30   ABEV    0.276186      0.076279  0.087504   \n",
      "31      U         NaN           NaN       NaN   \n",
      "32   LUMN    6.791626     46.126185  3.637539   \n",
      "33   AGNC    2.903338      8.429374  0.246199   \n",
      "34     VZ    6.101895     37.233121  0.150115   \n",
      "35    WBA   11.160624    124.559519  0.542626   \n",
      "36    WFC   11.388831    129.705466  0.232477   \n",
      "37   RIVN         NaN           NaN       NaN   \n",
      "38   UPST         NaN           NaN       NaN   \n",
      "39   GRAB         NaN           NaN       NaN   \n",
      "40   CSCO    4.427948     19.606722  0.080049   \n",
      "41   VALE    2.052698      4.213569  0.112738   \n",
      "42   AVGO   43.637612   1904.241165  0.285096   \n",
      "43    PBR    3.870208     14.978513  0.277655   \n",
      "44  GOOGL   26.245514    688.826994  0.197055   \n",
      "45   SMMT    8.332245     69.426307  1.485109   \n",
      "46   GOLD    3.213141     10.324273  0.147682   \n",
      "47    CMG   16.964175    287.783230  0.259016   \n",
      "48    BCS    1.930625      3.727313  0.156823   \n",
      "49    UAA    6.630928     43.969210  0.742896   \n",
      "\n",
      "                                     Predicted_Prices  \\\n",
      "0   23.47164366200724, 23.532782833562223, 23.6039...   \n",
      "1   36.78648930225639, 36.910824008131954, 36.9888...   \n",
      "2                                                       \n",
      "3   314.6238644678255, 314.2806046432679, 314.6773...   \n",
      "4   168.71119219664263, 168.71599491693482, 168.81...   \n",
      "5   3.2635603164801688, 3.2705157265398115, 3.2648...   \n",
      "6   17.922384999235504, 17.987801982208694, 17.962...   \n",
      "7                                                       \n",
      "8   11.482032712403306, 11.418953883275513, 11.337...   \n",
      "9                                                       \n",
      "10                                                      \n",
      "11  3.3134370928005126, 3.305182680367554, 3.30316...   \n",
      "12  17.188051979038324, 17.22247373067304, 17.2299...   \n",
      "13  15.156787645883215, 15.188550356765537, 15.135...   \n",
      "14  5.655264929765624, 5.646539702889784, 5.659604...   \n",
      "15  39.800906495537575, 39.872185629458514, 39.894...   \n",
      "16  13.324635179669196, 13.334308521726898, 13.235...   \n",
      "17  82.70639810945067, 82.70629570112153, 82.74077...   \n",
      "18  111.19241304637194, 111.28443054730417, 111.44...   \n",
      "19  54.73424520218254, 54.7721493833139, 54.870943...   \n",
      "20  3.826107686596477, 3.80980091310778, 3.7952772...   \n",
      "21  20.15073608881434, 20.17967077114723, 20.18585...   \n",
      "22  41.11183406728322, 41.25316797755577, 41.26256...   \n",
      "23                                                      \n",
      "24  17.04087684589312, 17.008260999452553, 16.9721...   \n",
      "25  64.35964627231147, 64.45318653082141, 64.41510...   \n",
      "26                                                      \n",
      "27  15.77004380526402, 15.759198779029772, 15.7103...   \n",
      "28  3.918056744739028, 3.976757949655269, 3.977222...   \n",
      "29  132.97201406439802, 132.84936443976355, 132.70...   \n",
      "30  2.828098430648669, 2.832414532436489, 2.830093...   \n",
      "31                                                      \n",
      "32  10.672618308322816, 10.664521824785988, 10.627...   \n",
      "33  11.795414698359684, 11.752737865799094, 11.709...   \n",
      "34  47.23982642861523, 47.302862413800945, 47.2484...   \n",
      "35  41.698231123905, 41.66856479642687, 41.5540427...   \n",
      "36  49.65084532772566, 49.714121479639594, 49.7355...   \n",
      "37                                                      \n",
      "38                                                      \n",
      "39                                                      \n",
      "40  51.09620254727972, 51.1287781153573, 51.087716...   \n",
      "41  14.165820498184885, 14.191163662620392, 14.160...   \n",
      "42  58.57820008554895, 58.62586683395538, 58.67586...   \n",
      "43  14.028951523524558, 14.099165321813347, 14.145...   \n",
      "44  133.18340197205575, 133.257220693871, 133.5194...   \n",
      "45  0.5813908985492466, 0.6157226455390923, 0.5894...   \n",
      "46  17.63248880996929, 17.604407343933694, 17.5802...   \n",
      "47  32.48724780762864, 32.44401764816422, 32.41982...   \n",
      "48  8.51703501387857, 8.520633919410736, 8.5268306...   \n",
      "49  12.425412999697382, 12.37528128442922, 12.3807...   \n",
      "\n",
      "                                        Actual_Prices  \\\n",
      "0   13.381999969482422, 13.175999641418455, 13.260...   \n",
      "1   29.440000534057617, 28.959999084472656, 28.469...   \n",
      "2                                                       \n",
      "3   309.07000732421875, 308.7300109863281, 300.799...   \n",
      "4   154.47999572753906, 156.89999389648438, 153.72...   \n",
      "5   3.7699999809265137, 3.890000104904175, 3.82999...   \n",
      "6   16.760000228881836, 16.559999465942383, 16.25,...   \n",
      "7                                                       \n",
      "8   13.15999984741211, 13.210000038146973, 12.2200...   \n",
      "9                                                       \n",
      "10                                                      \n",
      "11  3.369999885559082, 3.220000028610229, 3.150000...   \n",
      "12  14.93000030517578, 13.09000015258789, 13.05000...   \n",
      "13  14.210000038146973, 13.970000267028809, 13.229...   \n",
      "14  4.679999828338623, 4.590000152587891, 4.5, 4.4...   \n",
      "15  34.689998626708984, 34.16999816894531, 33.1500...   \n",
      "16  10.779999732971191, 10.420000076293944, 9.7100...   \n",
      "17  69.06999969482422, 68.11000061035156, 66.75, 6...   \n",
      "18  76.7699966430664, 75.25, 74.4800033569336, 69....   \n",
      "19  45.43999862670898, 44.77000045776367, 43.91999...   \n",
      "20  3.640000104904175, 3.509999990463257, 3.539999...   \n",
      "21  10.869999885559082, 10.260000228881836, 10.850...   \n",
      "22  38.970001220703125, 38.5, 37.58000183105469, 3...   \n",
      "23                                                      \n",
      "24  13.520000457763672, 13.140000343322754, 12.979...   \n",
      "25  52.09999847412109, 50.79999923706055, 50.09999...   \n",
      "26                                                      \n",
      "27  15.329999923706056, 15.1899995803833, 14.57999...   \n",
      "28  2.940000057220459, 2.7300000190734863, 2.68000...   \n",
      "29  124.66000366210938, 122.19000244140624, 118.54...   \n",
      "30  2.9600000381469727, 3.0, 2.950000047683716, 2....   \n",
      "31                                                      \n",
      "32  8.819999694824219, 8.5600004196167, 8.46000003...   \n",
      "33  11.270000457763672, 11.1899995803833, 11.02999...   \n",
      "34  41.2400016784668, 40.59000015258789, 39.479999...   \n",
      "35  34.150001525878906, 33.9900016784668, 33.43000...   \n",
      "36  44.31999969482422, 43.40999984741211, 42.27000...   \n",
      "37                                                      \n",
      "38                                                      \n",
      "39                                                      \n",
      "40  43.29999923706055, 42.58000183105469, 41.59000...   \n",
      "41  13.600000381469728, 13.399999618530272, 13.130...   \n",
      "42  50.18399810791016, 49.305999755859375, 48.2140...   \n",
      "43  13.520000457763672, 13.539999961853027, 13.430...   \n",
      "44  103.06999969482422, 101.13999938964844, 99.279...   \n",
      "45  1.3899999856948853, 1.3799999952316284, 1.5199...   \n",
      "46  15.5, 15.050000190734863, 15.18000030517578, 1...   \n",
      "47  34.10340118408203, 33.99259948730469, 33.30559...   \n",
      "48  7.960000038146973, 7.829999923706055, 7.530000...   \n",
      "49  8.59000015258789, 8.270000457763672, 8.0600004...   \n",
      "\n",
      "                             Future_Price_Predictions  \\\n",
      "0   23.695260328598557, 23.769606852041303, 23.471...   \n",
      "1   36.44896566484691, 36.513144042453675, 36.7864...   \n",
      "2                                                       \n",
      "3   317.70789579320206, 318.0771411153326, 314.623...   \n",
      "4   169.17593267468828, 169.18054648396625, 168.71...   \n",
      "5   3.3888800196523308, 3.3796390626647903, 3.2635...   \n",
      "6   17.722788144747593, 17.75448508079665, 17.9223...   \n",
      "7                                                       \n",
      "8   13.822004769047615, 13.79403789495695, 11.4820...   \n",
      "9                                                       \n",
      "10                                                      \n",
      "11  3.34377312885239, 3.337299848246026, 3.3134370...   \n",
      "12  17.854567580709276, 17.886878611984688, 17.188...   \n",
      "13  13.524020916784789, 13.523094708360146, 15.156...   \n",
      "14  5.766604362988143, 5.768063126254373, 5.655264...   \n",
      "15  39.80932843085151, 39.8625241330308, 39.800906...   \n",
      "16  13.412094046209958, 13.414738101476743, 13.324...   \n",
      "17  82.8178738968764, 82.87042836497513, 82.706398...   \n",
      "18  112.81194361812624, 112.9853542546202, 111.192...   \n",
      "19  55.3561632066105, 55.44864449637934, 54.734245...   \n",
      "20  4.0085229583972675, 3.9978565449424077, 3.8261...   \n",
      "21  19.360158813814273, 19.450517386447206, 20.150...   \n",
      "22  44.755402228275535, 44.84952088138768, 41.1118...   \n",
      "23                                                      \n",
      "24  16.391827095705004, 16.3762930376618, 17.04087...   \n",
      "25  62.09428934704878, 62.120260031846335, 64.3596...   \n",
      "26                                                      \n",
      "27  15.425911741831579, 15.448850110494345, 15.770...   \n",
      "28  4.013001611925363, 4.039846292068808, 3.918056...   \n",
      "29  134.91872114249193, 134.69720935417575, 132.97...   \n",
      "30  2.811730720463165, 2.8105888846616813, 2.82809...   \n",
      "31                                                      \n",
      "32  9.398391942040236, 9.384591044785449, 10.67261...   \n",
      "33  11.091044888430927, 11.074184397439533, 11.795...   \n",
      "34  47.235615729484444, 47.264430961294465, 47.239...   \n",
      "35  43.0434548736807, 43.057227937340016, 41.69823...   \n",
      "36  49.84372884383361, 49.91495257125081, 49.65084...   \n",
      "37                                                      \n",
      "38                                                      \n",
      "39                                                      \n",
      "40  49.951769341840404, 49.96877534573668, 51.0962...   \n",
      "41  15.191801212701472, 15.172177537266087, 14.165...   \n",
      "42  58.95676471621758, 59.03068461367867, 58.57820...   \n",
      "43  17.41758151363372, 17.450703475973153, 14.0289...   \n",
      "44  135.7546817905774, 135.88406029154177, 133.183...   \n",
      "45  -0.8622819212122315, -0.867578910205808, 0.581...   \n",
      "46  18.140131928816512, 18.125183284266278, 17.632...   \n",
      "47  32.65404540465377, 32.61326190450079, 32.48724...   \n",
      "48  9.190870095179344, 9.19167694622273, 8.5170350...   \n",
      "49  13.871743910958525, 13.901569898017692, 12.425...   \n",
      "\n",
      "                                         Train_Prices  \n",
      "0   0.4860000014305115, 0.4769999980926513, 0.4722...  \n",
      "1   35.16999816894531, 34.81999969482422, 34.70999...  \n",
      "2                                                      \n",
      "3   17.58799934387207, 17.288000106811523, 16.6686...  \n",
      "4   25.44750022888184, 25.239999771118164, 25.2649...  \n",
      "5   11.30854034423828, 10.99862289428711, 10.79889...  \n",
      "6   26.555891036987305, 26.79002952575684, 26.8126...  \n",
      "7                                                      \n",
      "8   39.970001220703125, 40.5, 39.34999847412109, 3...  \n",
      "9                                                      \n",
      "10                                                     \n",
      "11  2.0999999046325684, 2.190000057220459, 2.02999...  \n",
      "12  16.579999923706055, 16.649999618530273, 16.360...  \n",
      "13  38.04999923706055, 36.630001068115234, 35.5800...  \n",
      "14  8.630000114440918, 8.640000343322754, 8.609999...  \n",
      "15  17.040000915527344, 16.950000762939453, 17.030...  \n",
      "16  40.11000061035156, 40.720001220703125, 40.3400...  \n",
      "17  41.54999923706055, 39.79999923706055, 39.58000...  \n",
      "18  3.880000114440918, 3.809999942779541, 3.759999...  \n",
      "19  29.013282775878903, 28.842504501342773, 28.633...  \n",
      "20  3.640000104904175, 3.539999961853028, 3.430000...  \n",
      "21  115.36000061035156, 109.04000091552734, 108.48...  \n",
      "22  104.47000122070312, 103.20999908447266, 101.72...  \n",
      "23                                                     \n",
      "24  12.84000015258789, 12.579999923706056, 12.4600...  \n",
      "25  32.47999954223633, 31.6299991607666, 30.600000...  \n",
      "26                                                     \n",
      "27  36.95000076293945, 36.77000045776367, 36.79000...  \n",
      "28  35.150001525878906, 34.08000183105469, 33.6300...  \n",
      "29  16.25, 16.56599998474121, 16.225000381469727, ...  \n",
      "30  6.820000171661377, 6.909999847412109, 6.860000...  \n",
      "31                                                     \n",
      "32  40.52000045776367, 40.75, 40.790000915527344, ...  \n",
      "33  22.1299991607666, 22.459999084472656, 22.25, 2...  \n",
      "34  49.68999862670898, 50.34999847412109, 50.18000...  \n",
      "35  63.18000030517578, 62.880001068115234, 60.4199...  \n",
      "36  53.2400016784668, 53.36000061035156, 52.900001...  \n",
      "37                                                     \n",
      "38                                                     \n",
      "39                                                     \n",
      "40  25.21999931335449, 25.200000762939453, 24.9699...  \n",
      "41  12.31999969482422, 12.0, 11.4399995803833, 11....  \n",
      "42  8.95199966430664, 8.90999984741211, 8.71800041...  \n",
      "43  17.09000015258789, 16.780000686645508, 16.1100...  \n",
      "44  29.863500595092773, 30.270000457763672, 29.863...  \n",
      "45  10.1899995803833, 10.289999961853027, 10.53999...  \n",
      "46  16.0, 15.649999618530272, 15.40999984741211, 1...  \n",
      "47  13.286399841308594, 13.281800270080566, 13.121...  \n",
      "48  15.529999732971191, 15.380000114440918, 15.260...  \n",
      "49  34.78499984741211, 34.20500183105469, 33.23500...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "import logging\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"09bb980b-6cef-48c3-9aa5-63f3cbc9885e\")\n",
    "index = pc.Index(\"financial-data-index\")\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "logger.info(\"SentenceTransformer 'all-MiniLM-L6-v2' loaded successfully.\")\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2'\n",
    "\n",
    "# Folder to save the processed CSV files\n",
    "output_folder = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2'\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# List of stock symbols\n",
    "symbols = ['NVDA', 'INTC', 'PLTR', 'TSLA', 'AAPL', 'BBD', 'T', 'SOFI',\n",
    "    'WBD', 'SNAP', 'NIO', 'BTG', 'F', 'AAL', 'NOK', 'BAC',\n",
    "    'CCL', 'ORCL', 'AMD', 'PFE', 'KGC', 'MARA', 'SLB', 'NU',\n",
    "    'MPW', 'MU', 'LCID', 'NCLH', 'RIG', 'AMZN', 'ABEV', 'U',\n",
    "    'LUMN', 'AGNC', 'VZ', 'WBA', 'WFC', 'RIVN', 'UPST', 'GRAB',\n",
    "    'CSCO', 'VALE', 'AVGO', 'PBR', 'GOOGL', 'SMMT', 'GOLD',\n",
    "    'CMG', 'BCS', 'UAA']\n",
    "\n",
    "def fetch_and_save_stock_data(symbol, start_date='2014-09-18', end_date='2024-09-18'):\n",
    "    \"\"\"\n",
    "    Fetch stock data from Yahoo Finance, save to CSV, and upsert data into Pinecone.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch data from Yahoo Finance\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        \n",
    "        # Save data to CSV\n",
    "        csv_file_path = os.path.join(folder_path, f'{symbol}.csv')\n",
    "        stock_data.to_csv(csv_file_path)\n",
    "        logger.info(f\"Data for {symbol} saved to {csv_file_path}\")\n",
    "        \n",
    "        # Load CSV data\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Select necessary columns and drop NaN values\n",
    "        required_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            logger.error(f\"{csv_file_path} is missing one or more required columns: {required_columns}\")\n",
    "            return\n",
    "        \n",
    "        df = df[required_columns].dropna()\n",
    "\n",
    "        # Convert each row into a single string of concatenated data\n",
    "        text_data = df.apply(\n",
    "            lambda row: f\"Date: {row['Date']}, Open: {row['Open']}, High: {row['High']}, Low: {row['Low']}, Close: {row['Close']}, Adj Close: {row['Adj Close']}, Volume: {row['Volume']}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Generate embeddings for the stock data\n",
    "        embeddings = embedding_model.encode(text_data.tolist(), convert_to_numpy=True)\n",
    "        \n",
    "        # Prepare data for Pinecone upsert\n",
    "        vectors = [(f\"{symbol}_{i}\", embeddings[i].tolist(), {'symbol': symbol}) for i in range(len(embeddings))]\n",
    "        \n",
    "        # Upsert data into Pinecone index\n",
    "        try:\n",
    "            response = index.upsert(vectors)\n",
    "            logger.info(f\"Successfully upserted {len(vectors)} vectors for {symbol}.\")\n",
    "            logger.info(f\"Pinecone response: {response}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error upserting data into Pinecone for {symbol}: {e}\")\n",
    "        \n",
    "        # Save the processed DataFrame to a new CSV file in the output folder\n",
    "        processed_file_path = os.path.join(output_folder, f'processed_{symbol}.csv')\n",
    "        df.to_csv(processed_file_path, index=False)\n",
    "        logger.info(f\"Processed data saved to {processed_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data for {symbol}: {e}\")\n",
    "\n",
    "# List of expected CSV files\n",
    "csv_files = [f'{symbol}.csv' for symbol in symbols]\n",
    "\n",
    "# Check existing CSV files in the folder\n",
    "csv_files_in_folder = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Extract symbols from existing CSV filenames\n",
    "csv_symbols_in_folder = [f.split('.')[0] for f in csv_files_in_folder]\n",
    "\n",
    "# Identify missing symbols\n",
    "missing_symbols = [symbol for symbol in symbols if symbol not in csv_symbols_in_folder]\n",
    "\n",
    "# Print the result\n",
    "if missing_symbols:\n",
    "    print(f\"Các mã cổ phiếu bị thiếu: {missing_symbols}\")\n",
    "else:\n",
    "    print(\"Đã có đủ dữ liệu cho tất cả 50 mã cổ phiếu.\")\n",
    "\n",
    "# Import necessary libraries for forecasting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from prophet import Prophet  # Changed from fbprophet to prophet\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import math\n",
    "\n",
    "# Function to load stock data from CSV\n",
    "def load_stock_data(symbol):\n",
    "    file_path = os.path.join(folder_path, f'{symbol}.csv')\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "    df = df[['Date', 'Close']]\n",
    "    df = df.sort_values('Date')  # Sort by date ascending\n",
    "    return df\n",
    "\n",
    "# Function to prepare data for Prophet model\n",
    "def prepare_data_prophet(df):\n",
    "    # Rename columns to 'ds' and 'y' as required by Prophet\n",
    "    df_prophet = df.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "    return df_prophet\n",
    "\n",
    "# Function to process each symbol using Prophet\n",
    "def process_symbol(symbol):\n",
    "    print(f\"Processing {symbol}...\")\n",
    "    try:\n",
    "        df = load_stock_data(symbol)\n",
    "        df_prophet = prepare_data_prophet(df)\n",
    "        \n",
    "        # Split data into train (first 8 years) and test (last 2 years)\n",
    "        train_end_date = df_prophet['ds'].min() + pd.DateOffset(years=8)\n",
    "        train_df = df_prophet[df_prophet['ds'] <= train_end_date]\n",
    "        test_df = df_prophet[df_prophet['ds'] > train_end_date]\n",
    "        \n",
    "        # Initialize and fit Prophet model\n",
    "        model = Prophet(daily_seasonality=False, yearly_seasonality=True, weekly_seasonality=True)\n",
    "        model.fit(train_df)\n",
    "        \n",
    "        # Create dataframe for test period\n",
    "        future = test_df[['ds']].copy()\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        # Extract the predicted 'yhat'\n",
    "        predicted = forecast['yhat'].tolist()\n",
    "        actual = test_df['y'].tolist()\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        mse = mean_squared_error(actual, predicted)\n",
    "        rmse = math.sqrt(mse)\n",
    "        mape = mean_absolute_percentage_error(actual, predicted)\n",
    "        \n",
    "        # Forecast future prices for 1 year ahead\n",
    "        future_dates = model.make_future_dataframe(periods=365)\n",
    "        future_forecast = model.predict(future_dates)\n",
    "        future_prices = future_forecast['yhat'][-365:].tolist()\n",
    "        \n",
    "        # Train prices (actual training data)\n",
    "        train_prices = train_df['y'].tolist()\n",
    "        \n",
    "        return {\n",
    "            'Symbol': symbol,\n",
    "            'RMSE': rmse,\n",
    "            'MSE': mse,\n",
    "            'MAPE': mape,\n",
    "            'Predicted_Prices': predicted,\n",
    "            'Actual_Prices': actual,\n",
    "            'Future_Price_Predictions': future_prices,\n",
    "            'Train_Prices': train_prices\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing symbol {symbol} with Prophet: {e}\")\n",
    "        return {\n",
    "            'Symbol': symbol,\n",
    "            'RMSE': None,\n",
    "            'MSE': None,\n",
    "            'MAPE': None,\n",
    "            'Predicted_Prices': [],\n",
    "            'Actual_Prices': [],\n",
    "            'Future_Price_Predictions': [],\n",
    "            'Train_Prices': []\n",
    "        }\n",
    "\n",
    "# Main processing block\n",
    "if __name__ == '__main__':\n",
    "    # Create a list to store forecast results\n",
    "    forecast_results = []\n",
    "\n",
    "    # Parallel processing for all symbols\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_symbol)(symbol) for symbol in symbols\n",
    "    )\n",
    "\n",
    "    # Add the results to forecast_results list\n",
    "    forecast_results.extend(results)\n",
    "\n",
    "    # Create the summary DataFrame\n",
    "    summary_df = pd.DataFrame(forecast_results)\n",
    "\n",
    "    # Convert lists to strings for saving\n",
    "    summary_df['Predicted_Prices'] = summary_df['Predicted_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Actual_Prices'] = summary_df['Actual_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Future_Price_Predictions'] = summary_df['Future_Price_Predictions'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Train_Prices'] = summary_df['Train_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "\n",
    "    # Save the summary DataFrame to CSV with the specified filename\n",
    "    summary_file = os.path.join(output_folder, 'forecast_summary_Prophet.csv')  # Updated filename\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "    print(f\"Tất cả kết quả đã được lưu trong {summary_file}\")\n",
    "\n",
    "    # Print the summary DataFrame\n",
    "    print(\"\\nBảng tổng hợp kết quả dự báo:\")\n",
    "    print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b84b6cb-68ff-46bd-adde-701d4227af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 20:51:49,888 INFO:Loaded 'forecast_summary_Prophet.csv' successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15652e35df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dash\n",
    "from dash import html, dcc\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "import ast\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Paths to data\n",
    "folder_path = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2'  # Folder with stock CSV files\n",
    "output_folder = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2'  # Output folder\n",
    "\n",
    "# List of stock symbols\n",
    "symbols = ['NVDA', 'INTC', 'PLTR', 'TSLA', 'AAPL', 'BBD', 'T', 'SOFI',\n",
    "           'WBD', 'SNAP', 'NIO', 'BTG', 'F', 'AAL', 'NOK', 'BAC',\n",
    "           'CCL', 'ORCL', 'AMD', 'PFE', 'KGC', 'MARA', 'SLB', 'NU',\n",
    "           'MPW', 'MU', 'LCID', 'NCLH', 'RIG', 'AMZN', 'ABEV', 'U',\n",
    "           'LUMN', 'AGNC', 'VZ', 'WBA', 'WFC', 'RIVN', 'UPST', 'GRAB',\n",
    "           'CSCO', 'VALE', 'AVGO', 'PBR', 'GOOGL', 'SMMT', 'GOLD',\n",
    "           'CMG', 'BCS', 'UAA']\n",
    "\n",
    "# Load forecast_summary_Prophet.csv\n",
    "forecast_summary_file = os.path.join(output_folder, 'forecast_summary_Prophet.csv')\n",
    "if os.path.exists(forecast_summary_file):\n",
    "    forecast_summary_df = pd.read_csv(forecast_summary_file)\n",
    "    # Parse string lists into actual lists\n",
    "    for col in ['Predicted_Prices', 'Actual_Prices', 'Future_Price_Predictions', 'Train_Prices']:\n",
    "        if col in forecast_summary_df.columns:\n",
    "            forecast_summary_df[col] = forecast_summary_df[col].apply(\n",
    "                lambda x: [float(item.strip()) for item in x.split(',')] if pd.notnull(x) else []\n",
    "            )\n",
    "    logging.info(\"Loaded 'forecast_summary_Prophet.csv' successfully.\")\n",
    "else:\n",
    "    forecast_summary_df = pd.DataFrame()\n",
    "    logging.warning(\"Could not find 'forecast_summary_Prophet.csv'. Evaluation metrics and forecasts will not be displayed.\")\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server  # For deployment\n",
    "\n",
    "# Define app layout\n",
    "app.layout = html.Div([\n",
    "    html.H1('Stock Price Predictions Dashboard (Prophet)'),\n",
    "    html.Div([\n",
    "        html.Label('Select Stock Symbol'),\n",
    "        dcc.Dropdown(\n",
    "            id='stock-dropdown',\n",
    "            options=[{'label': s, 'value': s} for s in symbols],\n",
    "            value='NVDA'  # Default stock\n",
    "        )\n",
    "    ], style={'width': '25%', 'display': 'inline-block'}),\n",
    "    html.Div([\n",
    "        html.Label('Select Date Range'),\n",
    "        dcc.DatePickerRange(\n",
    "            id='date-picker',\n",
    "            min_date_allowed='2014-09-18',  # Adjusted min date for 10-year period\n",
    "            max_date_allowed='2025-12-31',  # Extended max date to accommodate future forecasts\n",
    "            start_date='2014-09-18',        # Default start date\n",
    "            end_date='2025-09-18'           # Extended default end date to include 1-year forecast\n",
    "        )\n",
    "    ], style={'display': 'inline-block', 'marginLeft': '50px'}),\n",
    "    html.Div([\n",
    "        html.Label('Show 1-Year Forecast'),\n",
    "        dcc.Checklist(\n",
    "            id='forecast-checkbox',\n",
    "            options=[{'label': 'Include 1-Year Forecast', 'value': 'show_forecast'}],\n",
    "            value=[],  # Default unchecked\n",
    "            inline=True\n",
    "        )\n",
    "    ], style={'marginTop': '20px'}),\n",
    "    dcc.Graph(id='price-graph'),\n",
    "    html.Div(id='metrics-output', style={'marginTop': '20px'})\n",
    "])\n",
    "\n",
    "# Define callback to update graph and evaluation metrics\n",
    "@app.callback(\n",
    "    [Output('price-graph', 'figure'),\n",
    "     Output('metrics-output', 'children')],\n",
    "    [Input('stock-dropdown', 'value'),\n",
    "     Input('date-picker', 'start_date'),\n",
    "     Input('date-picker', 'end_date'),\n",
    "     Input('forecast-checkbox', 'value')]\n",
    ")\n",
    "def update_graph(selected_stock, start_date, end_date, forecast_option):\n",
    "    # Convert start_date and end_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Get the forecast data for the selected stock\n",
    "    forecast_row = forecast_summary_df[forecast_summary_df['Symbol'] == selected_stock]\n",
    "\n",
    "    if forecast_row.empty:\n",
    "        logging.error(f\"No forecast data found for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"No forecast data found for {selected_stock}.\"\n",
    "\n",
    "    # Parse 'Actual_Prices', 'Predicted_Prices', 'Future_Price_Predictions', 'Train_Prices'\n",
    "    actual_prices = forecast_row.iloc[0]['Actual_Prices']\n",
    "    predicted_prices = forecast_row.iloc[0]['Predicted_Prices']\n",
    "    future_prices = forecast_row.iloc[0]['Future_Price_Predictions']\n",
    "    train_prices = forecast_row.iloc[0]['Train_Prices']\n",
    "\n",
    "    # Load the stock data to get dates\n",
    "    stock_data_file = os.path.join(folder_path, f'{selected_stock}.csv')\n",
    "    if not os.path.exists(stock_data_file):\n",
    "        logging.error(f\"Stock data file not found for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Stock data file not found for {selected_stock}.\"\n",
    "\n",
    "    df_stock = pd.read_csv(stock_data_file)\n",
    "    if 'Date' not in df_stock.columns or 'Close' not in df_stock.columns:\n",
    "        logging.error(f\"Incorrect data format in stock data file for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Incorrect data format in stock data file for {selected_stock}.\"\n",
    "\n",
    "    # Convert 'Date' to datetime\n",
    "    df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
    "    df_stock_sorted = df_stock.sort_values('Date')\n",
    "    dates = df_stock_sorted['Date']\n",
    "    data_length = len(df_stock_sorted)\n",
    "\n",
    "    # Derive split_date based on 8-year training period\n",
    "    split_date = df_stock_sorted['Date'].min() + pd.DateOffset(years=8)\n",
    "\n",
    "    # Get dates for Train and Test based on split_date\n",
    "    train_mask = df_stock_sorted['Date'] <= split_date\n",
    "    test_mask = df_stock_sorted['Date'] > split_date\n",
    "\n",
    "    df_train = df_stock_sorted[train_mask]\n",
    "    df_test = df_stock_sorted[test_mask]\n",
    "\n",
    "    # Ensure that 'Actual_Prices' and 'Predicted_Prices' correspond to the test set\n",
    "    test_dates = df_test['Date'].reset_index(drop=True)\n",
    "    actual_prices = actual_prices[:len(test_dates)]  # Trim if necessary\n",
    "    predicted_prices = predicted_prices[:len(test_dates)]\n",
    "\n",
    "    if len(actual_prices) != len(predicted_prices):\n",
    "        logging.error(f\"Length mismatch between actual and predicted prices for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Length mismatch in data for {selected_stock}.\"\n",
    "\n",
    "    # Create DataFrame for Train set within selected date range\n",
    "    df_train_filtered = df_train[(df_train['Date'] >= start_date) & (df_train['Date'] <= end_date)]\n",
    "\n",
    "    # Create DataFrame for Test set within selected date range\n",
    "    df_test_filtered = pd.DataFrame({\n",
    "        'Date': test_dates[:len(actual_prices)],\n",
    "        'Actual_Price': actual_prices,\n",
    "        'Predicted_Price': predicted_prices\n",
    "    })\n",
    "    df_test_filtered = df_test_filtered[\n",
    "        (df_test_filtered['Date'] >= start_date) & (df_test_filtered['Date'] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Initialize data list for plotting\n",
    "    data = []\n",
    "\n",
    "    # Add Train Prices if available in the selected date range\n",
    "    if not df_train_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_train_filtered['Date'],\n",
    "                y=df_train_filtered['Close'],\n",
    "                mode='lines',\n",
    "                name='Train Price',\n",
    "                line=dict(color='red')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add Actual Test Prices\n",
    "    if not df_test_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_test_filtered['Date'],\n",
    "                y=df_test_filtered['Actual_Price'],\n",
    "                mode='lines',\n",
    "                name='Actual Test Price',\n",
    "                line=dict(color='green')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add Predicted Test Prices\n",
    "    if not df_test_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_test_filtered['Date'],\n",
    "                y=df_test_filtered['Predicted_Price'],\n",
    "                mode='lines',\n",
    "                name='Predicted Test Price',\n",
    "                line=dict(color='blue')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add vertical line to indicate the split between train and test\n",
    "    if pd.notnull(split_date):\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=[split_date, split_date],\n",
    "                y=[df_stock_sorted['Close'].min(), df_stock_sorted['Close'].max()],\n",
    "                mode='lines',\n",
    "                name='Train-Test Split',\n",
    "                line=dict(color='black', dash='dash')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add future price predictions if selected\n",
    "    if 'show_forecast' in forecast_option:\n",
    "        # Generate future dates\n",
    "        last_date = df_stock_sorted['Date'].max()\n",
    "        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=len(future_prices), freq='D')\n",
    "\n",
    "        if len(future_prices) != len(future_dates):\n",
    "            logging.error(f\"Length mismatch between future prices and future dates for {selected_stock}.\")\n",
    "        else:\n",
    "            df_future = pd.DataFrame({\n",
    "                'Date': future_dates,\n",
    "                'Forecasted_Price': future_prices\n",
    "            })\n",
    "\n",
    "            # Filter future forecasts based on selected date range\n",
    "            df_future_filtered = df_future[\n",
    "                (df_future['Date'] >= start_date) & (df_future['Date'] <= end_date)\n",
    "            ]\n",
    "\n",
    "            if not df_future_filtered.empty:\n",
    "                data.append(\n",
    "                    go.Scatter(\n",
    "                        x=df_future_filtered['Date'],\n",
    "                        y=df_future_filtered['Forecasted_Price'],\n",
    "                        mode='lines',\n",
    "                        name='1-Year Forecast',\n",
    "                        line=dict(dash='dash', color='orange')\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                logging.warning(f\"No forecasted data within the selected date range for {selected_stock}.\")\n",
    "\n",
    "    # Create figure with dynamic title based on selected date range\n",
    "    figure = {\n",
    "        'data': data,\n",
    "        'layout': go.Layout(\n",
    "            title=f'{selected_stock} Price Prediction ({start_date.date()} to {end_date.date()})',\n",
    "            xaxis={'title': 'Date'},\n",
    "            yaxis={'title': 'Price'},\n",
    "            hovermode='closest'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Get RMSE, MSE, and MAPE\n",
    "    rmse = forecast_row.iloc[0]['RMSE']\n",
    "    mse = forecast_row.iloc[0]['MSE']\n",
    "    mape = forecast_row.iloc[0]['MAPE']\n",
    "\n",
    "    # Format metrics output\n",
    "    if pd.notnull(rmse) and pd.notnull(mse) and pd.notnull(mape):\n",
    "        metrics_output = [\n",
    "            html.P(f'Root Mean Squared Error (RMSE): {rmse:.4f}'),\n",
    "            html.P(f'Mean Squared Error (MSE): {mse:.4f}'),\n",
    "            html.P(f'Mean Absolute Percentage Error (MAPE): {mape:.2%}')\n",
    "        ]\n",
    "    else:\n",
    "        metrics_output = [html.P(\"Evaluation metrics are not available.\")]\n",
    "\n",
    "    return figure, metrics_output\n",
    "\n",
    "# Run Dash app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0fcce9-388c-478e-8e29-d88405b179c3",
   "metadata": {},
   "source": [
    "#XGBOOST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce157e5-3382-4eb1-9566-6471b5ac8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.1-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/124.9 MB 660.6 kB/s eta 0:03:10\n",
      "   ---------------------------------------- 0.1/124.9 MB 991.0 kB/s eta 0:02:06\n",
      "   ---------------------------------------- 0.3/124.9 MB 2.1 MB/s eta 0:00:59\n",
      "   ---------------------------------------- 0.8/124.9 MB 4.4 MB/s eta 0:00:29\n",
      "    --------------------------------------- 1.9/124.9 MB 8.1 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 4.2/124.9 MB 14.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 4.2/124.9 MB 14.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 4.2/124.9 MB 14.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 4.2/124.9 MB 14.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 4.2/124.9 MB 14.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 4.9/124.9 MB 9.3 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 6.2/124.9 MB 10.8 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 9.8/124.9 MB 15.7 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 13.3/124.9 MB 26.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 15.1/124.9 MB 46.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 17.7/124.9 MB 59.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 19.5/124.9 MB 59.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 20.7/124.9 MB 46.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 22.2/124.9 MB 38.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 24.1/124.9 MB 40.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 25.9/124.9 MB 36.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 26.6/124.9 MB 36.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 27.6/124.9 MB 32.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 28.1/124.9 MB 29.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 28.8/124.9 MB 25.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 29.1/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 29.4/124.9 MB 20.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 29.8/124.9 MB 20.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 30.0/124.9 MB 18.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 30.4/124.9 MB 17.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 30.7/124.9 MB 16.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 31.0/124.9 MB 15.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 31.3/124.9 MB 14.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 31.7/124.9 MB 13.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 32.0/124.9 MB 12.8 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 32.3/124.9 MB 12.8 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 32.6/124.9 MB 12.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 33.0/124.9 MB 11.5 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 33.3/124.9 MB 11.3 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 33.7/124.9 MB 10.9 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 34.0/124.9 MB 10.4 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 34.3/124.9 MB 9.8 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 34.7/124.9 MB 9.4 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 35.0/124.9 MB 9.0 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 35.4/124.9 MB 8.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 35.7/124.9 MB 8.6 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 36.1/124.9 MB 8.3 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 36.4/124.9 MB 8.1 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 7.8 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 37.1/124.9 MB 7.8 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 37.4/124.9 MB 7.5 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 37.8/124.9 MB 7.3 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 38.1/124.9 MB 7.3 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 38.5/124.9 MB 7.2 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 38.9/124.9 MB 7.1 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 39.2/124.9 MB 7.0 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 39.6/124.9 MB 7.1 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 39.9/124.9 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 40.3/124.9 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 40.6/124.9 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 41.0/124.9 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 41.3/124.9 MB 7.3 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 41.7/124.9 MB 7.3 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 42.0/124.9 MB 7.3 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 42.4/124.9 MB 7.4 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 42.8/124.9 MB 7.4 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 43.1/124.9 MB 7.4 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 43.5/124.9 MB 7.4 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 43.9/124.9 MB 7.4 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 44.2/124.9 MB 7.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 44.6/124.9 MB 7.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 45.0/124.9 MB 7.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 45.3/124.9 MB 7.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 45.7/124.9 MB 7.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 46.1/124.9 MB 7.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 46.4/124.9 MB 7.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 46.8/124.9 MB 7.5 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 47.2/124.9 MB 7.5 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 47.5/124.9 MB 7.6 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 47.9/124.9 MB 7.6 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 48.3/124.9 MB 7.6 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 48.7/124.9 MB 7.6 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 49.0/124.9 MB 7.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 49.4/124.9 MB 7.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 49.8/124.9 MB 7.7 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 50.2/124.9 MB 7.7 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 50.6/124.9 MB 7.8 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 51.0/124.9 MB 7.8 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 51.3/124.9 MB 7.8 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 51.7/124.9 MB 7.9 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 52.1/124.9 MB 7.9 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 52.5/124.9 MB 7.9 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 52.9/124.9 MB 7.9 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 53.3/124.9 MB 8.0 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 53.7/124.9 MB 8.0 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 54.1/124.9 MB 8.0 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 54.5/124.9 MB 8.0 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 54.9/124.9 MB 8.1 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 55.3/124.9 MB 8.1 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 55.6/124.9 MB 8.1 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 56.0/124.9 MB 8.2 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 56.5/124.9 MB 8.2 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 56.9/124.9 MB 8.2 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 57.3/124.9 MB 8.2 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 57.7/124.9 MB 8.3 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 58.1/124.9 MB 8.3 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 58.5/124.9 MB 8.3 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 58.9/124.9 MB 8.3 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 59.3/124.9 MB 8.3 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 59.7/124.9 MB 8.3 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 60.1/124.9 MB 8.4 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 60.5/124.9 MB 8.4 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 60.9/124.9 MB 8.4 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 61.4/124.9 MB 8.4 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 61.8/124.9 MB 8.4 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 62.2/124.9 MB 8.5 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 62.6/124.9 MB 8.5 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 63.0/124.9 MB 8.5 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 63.4/124.9 MB 8.6 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 63.9/124.9 MB 8.6 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 64.3/124.9 MB 8.6 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 64.7/124.9 MB 8.6 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 65.1/124.9 MB 8.6 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 65.6/124.9 MB 8.7 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 66.0/124.9 MB 8.7 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 66.4/124.9 MB 8.7 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 66.9/124.9 MB 8.8 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 67.3/124.9 MB 8.8 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 67.7/124.9 MB 8.8 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 68.2/124.9 MB 9.0 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 68.6/124.9 MB 9.0 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 69.0/124.9 MB 9.0 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 69.5/124.9 MB 9.0 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 69.9/124.9 MB 9.0 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 70.3/124.9 MB 9.1 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 70.8/124.9 MB 9.1 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 71.2/124.9 MB 9.1 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 71.7/124.9 MB 9.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 72.1/124.9 MB 9.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 72.6/124.9 MB 9.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 73.0/124.9 MB 9.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 73.5/124.9 MB 9.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 73.9/124.9 MB 9.1 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 74.8/124.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 75.3/124.9 MB 9.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 75.7/124.9 MB 9.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 76.2/124.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 76.6/124.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 77.1/124.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 77.5/124.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 78.0/124.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 78.5/124.9 MB 9.5 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 78.9/124.9 MB 9.5 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 79.4/124.9 MB 9.5 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 79.9/124.9 MB 9.5 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 80.3/124.9 MB 9.6 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 80.8/124.9 MB 9.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 81.3/124.9 MB 9.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 81.7/124.9 MB 9.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 82.2/124.9 MB 9.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 82.6/124.9 MB 9.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 83.1/124.9 MB 9.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 83.6/124.9 MB 9.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 84.1/124.9 MB 9.8 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 84.5/124.9 MB 9.8 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 85.0/124.9 MB 9.8 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 85.5/124.9 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 86.0/124.9 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 86.5/124.9 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 86.9/124.9 MB 9.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 87.5/124.9 MB 10.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 87.9/124.9 MB 9.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 88.4/124.9 MB 10.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 88.9/124.9 MB 10.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 89.4/124.9 MB 10.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 89.9/124.9 MB 10.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 90.4/124.9 MB 10.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 90.9/124.9 MB 10.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 91.3/124.9 MB 10.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 91.8/124.9 MB 10.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 92.4/124.9 MB 10.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 92.9/124.9 MB 10.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 93.3/124.9 MB 10.4 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 93.8/124.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 94.4/124.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 94.9/124.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 95.3/124.9 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 95.9/124.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 96.4/124.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 96.9/124.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 97.4/124.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 97.9/124.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 98.4/124.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 98.9/124.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 99.4/124.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 100.0/124.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 100.5/124.9 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 101.0/124.9 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 101.5/124.9 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 102.0/124.9 MB 10.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 102.6/124.9 MB 10.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 103.1/124.9 MB 10.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 103.6/124.9 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 104.1/124.9 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 104.7/124.9 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 105.2/124.9 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 105.7/124.9 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 106.3/124.9 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 106.8/124.9 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 107.3/124.9 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 107.8/124.9 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 108.4/124.9 MB 11.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 108.9/124.9 MB 11.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 109.4/124.9 MB 11.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 110.0/124.9 MB 11.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 110.5/124.9 MB 11.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 111.0/124.9 MB 11.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 111.6/124.9 MB 11.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 112.1/124.9 MB 11.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 112.7/124.9 MB 11.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 113.2/124.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 113.8/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.3/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.9/124.9 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 115.4/124.9 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 116.0/124.9 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 116.5/124.9 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 117.1/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 117.6/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 118.2/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 118.8/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.3/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.9/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 120.4/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.0/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.5/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  122.2/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  122.7/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.3/124.9 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.8/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.4/124.9 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 10.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d16c718-933e-4832-bccf-d1b2ea5048d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['C:\\\\Users\\\\Pham Ty\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:SentenceTransformer 'all-MiniLM-L6-v2' loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã có đủ dữ liệu cho tất cả 50 mã cổ phiếu.\n",
      "Tất cả kết quả đã được lưu trong C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2\\forecast_summary_XGBoost.csv\n",
      "\n",
      "Bảng tổng hợp kết quả dự báo:\n",
      "   Symbol       RMSE          MSE      MAPE  \\\n",
      "0    NVDA  43.837172  1921.697610  0.360515   \n",
      "1    INTC   2.244394     5.037303  0.047655   \n",
      "2    PLTR   1.360853     1.851922  0.037777   \n",
      "3    TSLA  13.349218   178.201608  0.056915   \n",
      "4    AAPL  20.832438   433.990474  0.066988   \n",
      "5     BBD   0.496065     0.246080  0.151817   \n",
      "6       T   0.682546     0.465869  0.030761   \n",
      "7    SOFI   0.289385     0.083744  0.028162   \n",
      "8     WBD   2.486245     6.181415  0.219172   \n",
      "9    SNAP   0.640605     0.410374  0.032621   \n",
      "10    NIO   0.360952     0.130286  0.038337   \n",
      "11    BTG   0.083994     0.007055  0.020210   \n",
      "12      F   0.309506     0.095794  0.018049   \n",
      "13    AAL   0.492877     0.242927  0.027849   \n",
      "14    NOK   0.109273     0.011941  0.020229   \n",
      "15    BAC   0.589656     0.347695  0.013198   \n",
      "16    CCL   0.627717     0.394029  0.038501   \n",
      "17   ORCL  20.228306   409.184375  0.117222   \n",
      "18    AMD  18.447103   340.295597  0.073667   \n",
      "19    PFE   0.908253     0.824924  0.019976   \n",
      "20    KGC   0.173941     0.030256  0.022415   \n",
      "21   MARA   1.452813     2.110664  0.075986   \n",
      "22    SLB   3.812233    14.533123  0.057988   \n",
      "23     NU   1.502490     2.257476  0.083543   \n",
      "24    MPW   4.665845    21.770107  0.803059   \n",
      "25     MU  16.208877   262.727710  0.073590   \n",
      "26   LCID   1.351295     1.825999  0.431511   \n",
      "27   NCLH   0.654182     0.427954  0.029666   \n",
      "28    RIG   0.239936     0.057569  0.031990   \n",
      "29   AMZN   6.291604    39.584276  0.027187   \n",
      "30   ABEV   0.072935     0.005319  0.021240   \n",
      "31      U   9.966450    99.330126  0.428891   \n",
      "32   LUMN   5.799628    33.635685  3.213692   \n",
      "33   AGNC   0.845608     0.715053  0.074968   \n",
      "34     VZ   3.487885    12.165343  0.071653   \n",
      "35    WBA  11.653762   135.810166  0.553593   \n",
      "36    WFC   0.953108     0.908415  0.015681   \n",
      "37   RIVN   2.397728     5.749098  0.186422   \n",
      "38   UPST   1.766064     3.118983  0.044036   \n",
      "39   GRAB   0.065236     0.004256  0.013599   \n",
      "40   CSCO   0.735914     0.541569  0.011082   \n",
      "41   VALE   0.325200     0.105755  0.017533   \n",
      "42   AVGO  50.110456  2511.057825  0.293022   \n",
      "43    PBR   0.506363     0.256403  0.024741   \n",
      "44  GOOGL  13.658530   186.555429  0.051575   \n",
      "45   SMMT   1.726016     2.979132  0.074323   \n",
      "46   GOLD   0.393322     0.154702  0.017415   \n",
      "47    CMG  12.315881   151.680919  0.154819   \n",
      "48    BCS   0.187614     0.035199  0.016674   \n",
      "49    UAA   0.745051     0.555100  0.080351   \n",
      "\n",
      "                                     Predicted_Prices  \\\n",
      "0   12.423702239990234, 12.35080337524414, 12.8971...   \n",
      "1   26.47231674194336, 26.63661003112793, 27.76728...   \n",
      "2   18.329679489135742, 18.21915626525879, 18.2763...   \n",
      "3   264.0494689941406, 250.39303588867188, 242.625...   \n",
      "4   141.73269653320312, 141.00885009765625, 145.72...   \n",
      "5   3.726912498474121, 3.9718570709228516, 4.05074...   \n",
      "6   15.732903480529785, 15.93337345123291, 16.0171...   \n",
      "7   8.291360855102539, 8.366497993469238, 8.297675...   \n",
      "8   11.825222969055176, 12.323413848876953, 11.965...   \n",
      "9   10.939708709716797, 11.122010231018066, 11.181...   \n",
      "10  10.327887535095215, 11.366083145141602, 11.666...   \n",
      "11  3.197331428527832, 3.341860771179199, 3.406540...   \n",
      "12  11.35612678527832, 11.384055137634277, 11.9319...   \n",
      "13  12.219671249389648, 12.193831443786621, 13.177...   \n",
      "14  4.281666278839111, 4.291867733001709, 4.584238...   \n",
      "15  30.833620071411133, 31.089147567749023, 32.389...   \n",
      "16  9.063122749328613, 9.33749771118164, 8.9418363...   \n",
      "17  61.2327995300293, 62.569149017333984, 64.97855...   \n",
      "18  67.2201156616211, 63.39463424682617, 64.153457...   \n",
      "19  44.03429412841797, 43.95231628417969, 43.97727...   \n",
      "20  3.728501796722412, 3.933892250061035, 4.033752...   \n",
      "21  10.075905799865723, 10.709454536437988, 11.958...   \n",
      "22  35.9102668762207, 38.364295959472656, 38.80786...   \n",
      "23  11.451353073120117, 11.38929557800293, 11.2374...   \n",
      "24  11.904988288879395, 11.456802368164062, 11.998...   \n",
      "25  50.13534927368164, 51.74205017089844, 53.35893...   \n",
      "26  5.018866539001465, 4.820806980133057, 4.820806...   \n",
      "27  11.900100708007812, 11.92708683013916, 13.5289...   \n",
      "28  2.21061372756958, 2.587111234664917, 2.7804057...   \n",
      "29  113.48519897460938, 114.80441284179688, 118.09...   \n",
      "30  2.8213207721710205, 3.0630738735198975, 3.1024...   \n",
      "31  37.755210876464844, 38.77394104003906, 38.8257...   \n",
      "32  7.841777324676514, 8.057114601135254, 8.099418...   \n",
      "33  8.977578163146973, 9.142556190490723, 9.378451...   \n",
      "34  39.10147476196289, 39.12240982055664, 39.13734...   \n",
      "35  32.0062141418457, 32.23369216918945, 33.005889...   \n",
      "36  40.384056091308594, 41.45320510864258, 43.1660...   \n",
      "37  12.851311683654785, 12.8925199508667, 12.42392...   \n",
      "38  34.1305046081543, 33.97466278076172, 33.340381...   \n",
      "39  3.367560386657715, 3.343012571334839, 3.274925...   \n",
      "40  39.93296813964844, 40.6734619140625, 41.522762...   \n",
      "41  13.379018783569336, 14.087955474853516, 14.519...   \n",
      "42  44.61852264404297, 44.196128845214844, 47.2698...   \n",
      "43  12.452228546142578, 14.014472961425781, 13.876...   \n",
      "44  98.20509338378906, 98.2074966430664, 101.86888...   \n",
      "45  1.0842455625534058, 1.0501047372817993, 1.0452...   \n",
      "46  15.448235511779785, 15.808686256408691, 15.809...   \n",
      "47  29.68238639831543, 29.390100479125977, 29.8378...   \n",
      "48  5.992753982543945, 6.629458427429199, 7.139247...   \n",
      "49  7.772491455078125, 7.669369697570801, 7.794378...   \n",
      "\n",
      "                                        Actual_Prices  \\\n",
      "0   12.51200008392334, 13.166999816894531, 13.2089...   \n",
      "1   26.96999931335449, 27.700000762939453, 27.6399...   \n",
      "2   17.84000015258789, 17.950000762939453, 17.25, ...   \n",
      "3   242.3999938964844, 249.44000244140625, 240.809...   \n",
      "4   142.4499969482422, 146.10000610351562, 146.399...   \n",
      "5   4.059999942779541, 4.090000152587891, 4.119999...   \n",
      "6   15.899999618530272, 16.09000015258789, 15.9300...   \n",
      "7   8.529999732971191, 8.319999694824219, 8.479999...   \n",
      "8   11.949999809265137, 12.359999656677246, 12.220...   \n",
      "9   11.210000038146973, 11.239999771118164, 11.0, ...   \n",
      "10  11.729999542236328, 11.90999984741211, 13.1700...   \n",
      "11  3.380000114440918, 3.450000047683716, 3.450000...   \n",
      "12  11.470000267028809, 12.359999656677246, 12.510...   \n",
      "13  11.920000076293944, 12.949999809265137, 12.869...   \n",
      "14  4.389999866485596, 4.610000133514404, 4.559999...   \n",
      "15  31.09000015258789, 32.380001068115234, 31.9200...   \n",
      "16  6.849999904632568, 7.760000228881836, 7.429999...   \n",
      "17  63.08000183105469, 65.69000244140625, 66.63999...   \n",
      "18  66.11000061035156, 67.9000015258789, 67.940002...   \n",
      "19  44.13999938964844, 44.459999084472656, 44.1199...   \n",
      "20  3.950000047683716, 4.010000228881836, 3.930000...   \n",
      "21  11.079999923706056, 12.880000114440918, 12.840...   \n",
      "22  38.29999923706055, 39.119998931884766, 41.5699...   \n",
      "23  11.609999656677246, 11.789999961853027, 11.850...   \n",
      "24  11.550000190734863, 11.989999771118164, 11.399...   \n",
      "25  51.720001220703125, 53.959999084472656, 54.729...   \n",
      "26  4.769999980926514, 4.730000019073486, 4.719999...   \n",
      "27  11.399999618530272, 13.31999969482422, 13.2200...   \n",
      "28  2.6700000762939453, 2.900000095367432, 3.0, 3....   \n",
      "29  115.87999725341795, 121.08999633789062, 120.94...   \n",
      "30  3.109999895095825, 3.0999999046325684, 3.06999...   \n",
      "31  39.36000061035156, 39.310001373291016, 40.9399...   \n",
      "32  7.78000020980835, 8.039999961853027, 7.2800002...   \n",
      "33  8.520000457763672, 9.109999656677246, 8.5, 8.1...   \n",
      "34  39.15999984741211, 39.810001373291016, 39.4000...   \n",
      "35  32.43000030517578, 33.38999938964844, 33.15999...   \n",
      "36  41.58000183105469, 43.540000915527344, 43.3100...   \n",
      "37  12.6899995803833, 12.369999885559082, 11.71000...   \n",
      "38  33.95000076293945, 33.2400016784668, 35.540000...   \n",
      "39  3.299999952316284, 3.2850000858306885, 3.25, 3...   \n",
      "40  41.290000915527344, 41.81999969482422, 41.9900...   \n",
      "41  14.25, 14.640000343322754, 14.800000190734863,...   \n",
      "42  45.678001403808594, 47.98600006103516, 48.4799...   \n",
      "43  13.899999618530272, 13.670000076293944, 14.109...   \n",
      "44  98.63999938964844, 101.63999938964844, 101.430...   \n",
      "45  1.0399999618530271, 1.0299999713897705, 1.0499...   \n",
      "46  15.920000076293944, 16.049999237060547, 15.899...   \n",
      "47  30.0939998626709, 30.722999572753903, 30.39559...   \n",
      "48  6.610000133514404, 6.960000038146973, 6.780000...   \n",
      "49  7.03000020980835, 7.400000095367432, 7.4800000...   \n",
      "\n",
      "                             Future_Price_Predictions  \\\n",
      "0   29.23195, 29.23195, 29.23195, 29.23195, 29.231...   \n",
      "1   27.835802, 27.870167, 27.894651, 27.944628, 27...   \n",
      "2   31.827751, 33.410816, 33.675625, 33.67224, 34....   \n",
      "3   228.55606, 229.23062, 230.7759, 228.72444, 226...   \n",
      "4   171.2385, 171.2385, 171.10834, 171.13814, 170....   \n",
      "5   3.3083422, 3.2828753, 3.2828753, 3.2828753, 3....   \n",
      "6   22.140347, 21.463257, 21.462643, 21.384571, 21...   \n",
      "7   7.9801087, 7.712729, 7.407224, 7.2272267, 7.26...   \n",
      "8   12.104748, 12.104748, 12.104748, 12.104748, 12...   \n",
      "9   9.7273855, 9.369014, 9.369216, 8.904329, 8.990...   \n",
      "10  5.579651, 5.663633, 5.162797, 5.653094, 5.5730...   \n",
      "11  3.275068, 3.0853777, 3.0104003, 2.707874, 2.73...   \n",
      "12  10.785834, 10.572663, 10.576477, 10.487065, 10...   \n",
      "13  10.889891, 10.895086, 10.8390875, 10.828922, 1...   \n",
      "14  4.241677, 4.241677, 4.2596707, 4.199744, 4.198...   \n",
      "15  39.242382, 38.566086, 38.745117, 38.753746, 39...   \n",
      "16  17.311981, 17.261599, 17.015593, 16.220192, 15...   \n",
      "17  99.02189, 99.02189, 99.02189, 99.02189, 99.021...   \n",
      "18  138.22026, 138.18752, 140.37358, 138.48668, 13...   \n",
      "19  29.989246, 29.194275, 29.07398, 29.354742, 29....   \n",
      "20  9.200324, 9.226762, 9.02369, 8.674326, 8.89282...   \n",
      "21  14.953843, 14.953843, 14.953843, 15.100933, 15...   \n",
      "22  40.36843, 39.74508, 39.608704, 39.836826, 39.7...   \n",
      "23  11.4160385, 11.4160385, 11.4160385, 11.4160385...   \n",
      "24  10.8015375, 10.8015375, 10.8015375, 10.8015375...   \n",
      "25  83.69338, 85.52055, 83.287476, 84.23889, 84.05...   \n",
      "26  4.4087086, 4.4243393, 4.4087086, 4.4087086, 4....   \n",
      "27  19.835695, 19.16257, 18.450468, 18.390812, 17....   \n",
      "28  4.532143, 4.1389527, 4.1389527, 4.1389527, 3.9...   \n",
      "29  177.41019, 176.79883, 177.1387, 177.16602, 179...   \n",
      "30  2.3747988, 2.3747988, 2.4285305, 2.4471846, 2....   \n",
      "31  31.542408, 31.542408, 31.542408, 31.372768, 31...   \n",
      "32  8.249258, 8.249258, 8.249258, 8.249258, 8.2492...   \n",
      "33  10.753518, 10.742379, 10.753518, 10.742379, 10...   \n",
      "34  45.39896, 46.337837, 43.06595, 42.49369, 42.29...   \n",
      "35  33.525658, 33.525658, 33.525658, 33.525658, 33...   \n",
      "36  53.608944, 52.38327, 51.71208, 54.191742, 54.2...   \n",
      "37  13.837587, 13.761211, 13.735647, 13.980664, 13...   \n",
      "38  34.190907, 37.359093, 36.044506, 34.105465, 33...   \n",
      "39  3.4758627, 3.4728386, 3.472404, 3.475428, 3.38...   \n",
      "40  50.56377, 49.23828, 48.984585, 48.90122, 48.26...   \n",
      "41  10.555518, 10.492369, 10.343795, 10.205939, 9....   \n",
      "42  63.17904, 63.17904, 63.17904, 63.17904, 63.179...   \n",
      "43  14.662159, 14.503904, 14.177812, 14.4365425, 1...   \n",
      "44  142.74074, 142.79012, 142.64275, 142.64275, 14...   \n",
      "45  12.243961, 12.707225, 13.35902, 13.218002, 13....   \n",
      "46  20.742304, 20.866074, 20.570402, 19.846928, 19...   \n",
      "47  37.047127, 37.111996, 37.111996, 37.111996, 37...   \n",
      "48  11.90898, 11.678611, 11.477073, 11.365458, 11....   \n",
      "49  7.9438796, 7.9438796, 7.860294, 7.863085, 7.86...   \n",
      "\n",
      "                                         Train_Prices  \n",
      "0   0.4907500147819519, 0.4892500042915344, 0.4837...  \n",
      "1   36.22999954223633, 35.91999816894531, 35.56000...  \n",
      "2   27.75, 25.6299991607666, 24.65999984741211, 25...  \n",
      "3   13.800000190734863, 13.602666854858398, 13.187...  \n",
      "4   27.4325008392334, 27.0575008392334, 26.6875, 2...  \n",
      "5   8.836088180541992, 8.443526268005371, 8.264463...  \n",
      "6   24.29002952575684, 24.35800552368164, 24.27492...  \n",
      "7   17.149999618530273, 17.270000457763672, 16.719...  \n",
      "8   33.380001068115234, 32.869998931884766, 33.189...  \n",
      "9   21.21999931335449, 21.450000762939453, 21.2099...  \n",
      "10  6.989999771118164, 7.03000020980835, 7.0799999...  \n",
      "11  1.7300000190734863, 1.5199999809265137, 1.5499...  \n",
      "12  14.989999771118164, 14.279999732971191, 14.090...  \n",
      "13  49.970001220703125, 50.84000015258789, 47.9599...  \n",
      "14  7.869999885559082, 7.739999771118164, 7.730000...  \n",
      "15  17.1299991607666, 16.850000381469727, 16.71999...  \n",
      "16  42.95000076293945, 43.18999862670898, 43.11999...  \n",
      "17  39.95000076293945, 41.11000061035156, 40.63000...  \n",
      "18  2.5199999809265137, 2.470000028610229, 2.5, 2....  \n",
      "19  29.36432647705078, 29.2789363861084, 29.098672...  \n",
      "20  2.9600000381469727, 2.700000047683716, 2.59999...  \n",
      "21  138.72000122070312, 144.47999572753906, 149.03...  \n",
      "22  80.0, 79.9000015258789, 80.19000244140625, 82....  \n",
      "23  7.579999923706055, 7.960000038146973, 7.050000...  \n",
      "24  13.539999961853027, 13.239999771118164, 13.060...  \n",
      "25  34.0, 33.349998474121094, 32.91999816894531, 3...  \n",
      "26  9.979999542236328, 9.970000267028809, 9.944999...  \n",
      "27  43.790000915527344, 43.7400016784668, 43.75999...  \n",
      "28  16.469999313354492, 16.25, 16.780000686645508,...  \n",
      "29  15.366000175476074, 15.303500175476074, 14.753...  \n",
      "30  5.860000133514404, 5.75, 5.679999828338623, 5....  \n",
      "31  146.72000122070312, 144.5800018310547, 146.690...  \n",
      "32  37.58000183105469, 37.77999877929688, 37.88999...  \n",
      "33  22.239999771118164, 21.86000061035156, 21.7800...  \n",
      "34  45.58000183105469, 45.41999816894531, 45.52999...  \n",
      "35  74.5, 73.44000244140625, 72.38999938964844, 71...  \n",
      "36  53.70000076293945, 53.18999862670898, 52.77000...  \n",
      "37  58.36000061035156, 60.95000076293945, 64.52999...  \n",
      "38  60.04999923706055, 60.790000915527344, 115.089...  \n",
      "39  12.890000343322754, 12.699999809265137, 11.380...  \n",
      "40  26.86000061035156, 26.68000030517578, 26.59000...  \n",
      "41  7.190000057220459, 6.860000133514404, 7.039999...  \n",
      "42  9.920000076293944, 9.821000099182127, 9.732999...  \n",
      "43  7.110000133514404, 6.260000228881836, 6.280000...  \n",
      "44  26.07550048828125, 25.79199981689453, 24.90800...  \n",
      "45  10.697999954223633, 10.510000228881836, 10.260...  \n",
      "46  11.470000267028809, 10.449999809265137, 10.319...  \n",
      "47  13.153400421142578, 12.97659969329834, 12.6703...  \n",
      "48  14.460000038146973, 14.109999656677246, 14.229...  \n",
      "49  34.415000915527344, 34.27000045776367, 33.2249...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "import logging\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"09bb980b-6cef-48c3-9aa5-63f3cbc9885e\")\n",
    "index = pc.Index(\"financial-data-index\")\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "logger.info(\"SentenceTransformer 'all-MiniLM-L6-v2' loaded successfully.\")\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2'\n",
    "\n",
    "# Folder to save the processed CSV files\n",
    "output_folder = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2'\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# List of stock symbols\n",
    "symbols = ['NVDA', 'INTC', 'PLTR', 'TSLA', 'AAPL', 'BBD', 'T', 'SOFI',\n",
    "           'WBD', 'SNAP', 'NIO', 'BTG', 'F', 'AAL', 'NOK', 'BAC',\n",
    "           'CCL', 'ORCL', 'AMD', 'PFE', 'KGC', 'MARA', 'SLB', 'NU',\n",
    "           'MPW', 'MU', 'LCID', 'NCLH', 'RIG', 'AMZN', 'ABEV', 'U',\n",
    "           'LUMN', 'AGNC', 'VZ', 'WBA', 'WFC', 'RIVN', 'UPST', 'GRAB',\n",
    "           'CSCO', 'VALE', 'AVGO', 'PBR', 'GOOGL', 'SMMT', 'GOLD',\n",
    "           'CMG', 'BCS', 'UAA']\n",
    "\n",
    "def fetch_and_save_stock_data(symbol, start_date='2014-09-18', end_date='2024-09-18'):\n",
    "    \"\"\"\n",
    "    Fetch stock data from Yahoo Finance, save to CSV, and upsert data into Pinecone.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch data from Yahoo Finance\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        \n",
    "        # Save data to CSV\n",
    "        csv_file_path = os.path.join(folder_path, f'{symbol}.csv')\n",
    "        stock_data.to_csv(csv_file_path)\n",
    "        logger.info(f\"Data for {symbol} saved to {csv_file_path}\")\n",
    "        \n",
    "        # Load CSV data\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Select necessary columns and drop NaN values\n",
    "        required_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            logger.error(f\"{csv_file_path} is missing one or more required columns: {required_columns}\")\n",
    "            return\n",
    "        \n",
    "        df = df[required_columns].dropna()\n",
    "\n",
    "        # Convert each row into a single string of concatenated data\n",
    "        text_data = df.apply(\n",
    "            lambda row: f\"Date: {row['Date']}, Open: {row['Open']}, High: {row['High']}, Low: {row['Low']}, Close: {row['Close']}, Adj Close: {row['Adj Close']}, Volume: {row['Volume']}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Generate embeddings for the stock data\n",
    "        embeddings = embedding_model.encode(text_data.tolist(), convert_to_numpy=True)\n",
    "        \n",
    "        # Prepare data for Pinecone upsert\n",
    "        vectors = [(f\"{symbol}_{i}\", embeddings[i].tolist(), {'symbol': symbol}) for i in range(len(embeddings))]\n",
    "        \n",
    "        # Upsert data into Pinecone index\n",
    "        try:\n",
    "            response = index.upsert(vectors)\n",
    "            logger.info(f\"Successfully upserted {len(vectors)} vectors for {symbol}.\")\n",
    "            logger.info(f\"Pinecone response: {response}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error upserting data into Pinecone for {symbol}: {e}\")\n",
    "        \n",
    "        # Save the processed DataFrame to a new CSV file in the output folder\n",
    "        processed_file_path = os.path.join(output_folder, f'processed_{symbol}.csv')\n",
    "        df.to_csv(processed_file_path, index=False)\n",
    "        logger.info(f\"Processed data saved to {processed_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data for {symbol}: {e}\")\n",
    "\n",
    "# List of expected CSV files\n",
    "csv_files = [f'{symbol}.csv' for symbol in symbols]\n",
    "\n",
    "# Check existing CSV files in the folder\n",
    "csv_files_in_folder = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Extract symbols from existing CSV filenames\n",
    "csv_symbols_in_folder = [f.split('.')[0] for f in csv_files_in_folder]\n",
    "\n",
    "# Identify missing symbols\n",
    "missing_symbols = [symbol for symbol in symbols if symbol not in csv_symbols_in_folder]\n",
    "\n",
    "# Print the result\n",
    "if missing_symbols:\n",
    "    print(f\"Các mã cổ phiếu bị thiếu: {missing_symbols}\")\n",
    "else:\n",
    "    print(\"Đã có đủ dữ liệu cho tất cả 50 mã cổ phiếu.\")\n",
    "\n",
    "# Import necessary libraries for forecasting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import math\n",
    "import xgboost as xgb  # Import XGBoost\n",
    "\n",
    "# Function to load stock data from CSV\n",
    "def load_stock_data(symbol):\n",
    "    file_path = os.path.join(folder_path, f'{symbol}.csv')\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "    df = df[['Date', 'Close']]\n",
    "    df = df.sort_values('Date')  # Sort by date ascending\n",
    "    return df\n",
    "\n",
    "# Function to create lag features\n",
    "def create_lag_features(df, lag=60):\n",
    "    \"\"\"\n",
    "    Create lag features for time series forecasting.\n",
    "    \"\"\"\n",
    "    for i in range(1, lag + 1):\n",
    "        df[f'lag_{i}'] = df['Close'].shift(i)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Function to prepare data for XGBoost model\n",
    "def prepare_data_xgboost(df, lag=60):\n",
    "    \"\"\"\n",
    "    Prepare data with lag features for XGBoost.\n",
    "    \"\"\"\n",
    "    df_lag = create_lag_features(df, lag=lag)\n",
    "    X = df_lag[[f'lag_{i}' for i in range(1, lag + 1)]].values\n",
    "    y = df_lag['Close'].values\n",
    "    return X, y\n",
    "\n",
    "# Function to forecast future prices using XGBoost\n",
    "def forecast_future_prices(model, last_features, periods=252):  # Changed periods to 252 for business days\n",
    "    \"\"\"\n",
    "    Forecast future prices using the trained XGBoost model.\n",
    "    \"\"\"\n",
    "    forecast = []\n",
    "    features = last_features.copy()\n",
    "    for _ in range(periods):\n",
    "        pred = model.predict(features.reshape(1, -1))[0]\n",
    "        forecast.append(pred)\n",
    "        # Update features: remove the oldest lag and append the new prediction\n",
    "        features = np.roll(features, -1)\n",
    "        features[-1] = pred\n",
    "    return forecast\n",
    "\n",
    "# Function to process each symbol using XGBoost\n",
    "def process_symbol_xgboost(symbol, lag=60):\n",
    "    print(f\"Processing {symbol} with XGBoost...\")\n",
    "    try:\n",
    "        df = load_stock_data(symbol)\n",
    "        \n",
    "        # Prepare data with lag features\n",
    "        X, y = prepare_data_xgboost(df, lag=lag)\n",
    "        \n",
    "        # Split data into train (first 80%) and test (last 20%)\n",
    "        split_index = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split_index], X[split_index:]\n",
    "        y_train, y_test = y[:split_index], y[split_index:]\n",
    "        \n",
    "        # Initialize and train XGBoost model\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = math.sqrt(mse)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        \n",
    "        # Forecast future prices for 1 year ahead (252 business days)\n",
    "        last_features = X[-1]  # Last available features from the training data\n",
    "        future_prices = forecast_future_prices(model, last_features, periods=252)  # Changed to 252\n",
    "        \n",
    "        # Get actual test prices\n",
    "        actual = y_test.tolist()\n",
    "        \n",
    "        # Get predicted test prices\n",
    "        predicted = y_pred.tolist()\n",
    "        \n",
    "        # Get train prices\n",
    "        train_prices = y_train.tolist()\n",
    "        \n",
    "        return {\n",
    "            'Symbol': symbol,\n",
    "            'RMSE': rmse,\n",
    "            'MSE': mse,\n",
    "            'MAPE': mape,\n",
    "            'Predicted_Prices': predicted,\n",
    "            'Actual_Prices': actual,\n",
    "            'Future_Price_Predictions': future_prices,\n",
    "            'Train_Prices': train_prices\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing symbol {symbol} with XGBoost: {e}\")\n",
    "        return {\n",
    "            'Symbol': symbol,\n",
    "            'RMSE': None,\n",
    "            'MSE': None,\n",
    "            'MAPE': None,\n",
    "            'Predicted_Prices': [],\n",
    "            'Actual_Prices': [],\n",
    "            'Future_Price_Predictions': [],\n",
    "            'Train_Prices': []\n",
    "        }\n",
    "\n",
    "# Main processing block\n",
    "if __name__ == '__main__':\n",
    "    # Create a list to store forecast results\n",
    "    forecast_results = []\n",
    "\n",
    "    # Parallel processing for all symbols\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_symbol_xgboost)(symbol) for symbol in symbols\n",
    "    )\n",
    "\n",
    "    # Add the results to forecast_results list\n",
    "    forecast_results.extend(results)\n",
    "\n",
    "    # Create the summary DataFrame\n",
    "    summary_df = pd.DataFrame(forecast_results)\n",
    "\n",
    "    # Convert lists to strings for saving\n",
    "    summary_df['Predicted_Prices'] = summary_df['Predicted_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Actual_Prices'] = summary_df['Actual_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Future_Price_Predictions'] = summary_df['Future_Price_Predictions'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "    summary_df['Train_Prices'] = summary_df['Train_Prices'].apply(\n",
    "        lambda x: ', '.join(map(str, x))\n",
    "    )\n",
    "\n",
    "    # Save the summary DataFrame to CSV with the specified filename\n",
    "    summary_file = os.path.join(output_folder, 'forecast_summary_XGBoost.csv')  # Updated filename\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "    print(f\"Tất cả kết quả đã được lưu trong {summary_file}\")\n",
    "\n",
    "    # Print the summary DataFrame\n",
    "    print(\"\\nBảng tổng hợp kết quả dự báo:\")\n",
    "    print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af922cb-ea9a-416a-b99c-5dd8beebd87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 20:51:06,508 INFO:Loaded 'forecast_summary_XGBoost.csv' successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15653268a40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dash\n",
    "from dash import html, dcc\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Paths to data\n",
    "folder_path = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset-2'  # Folder with stock CSV files\n",
    "output_folder = r'C:\\Users\\Pham Ty\\Desktop\\Thesis-Predict\\Dataset_processed_2'  # Output folder\n",
    "\n",
    "# List of stock symbols\n",
    "symbols = ['NVDA', 'INTC', 'PLTR', 'TSLA', 'AAPL', 'BBD', 'T', 'SOFI',\n",
    "           'WBD', 'SNAP', 'NIO', 'BTG', 'F', 'AAL', 'NOK', 'BAC',\n",
    "           'CCL', 'ORCL', 'AMD', 'PFE', 'KGC', 'MARA', 'SLB', 'NU',\n",
    "           'MPW', 'MU', 'LCID', 'NCLH', 'RIG', 'AMZN', 'ABEV', 'U',\n",
    "           'LUMN', 'AGNC', 'VZ', 'WBA', 'WFC', 'RIVN', 'UPST', 'GRAB',\n",
    "           'CSCO', 'VALE', 'AVGO', 'PBR', 'GOOGL', 'SMMT', 'GOLD',\n",
    "           'CMG', 'BCS', 'UAA']\n",
    "\n",
    "# Load forecast_summary_XGBoost.csv\n",
    "forecast_summary_file = os.path.join(output_folder, 'forecast_summary_XGBoost.csv')\n",
    "if os.path.exists(forecast_summary_file):\n",
    "    forecast_summary_df = pd.read_csv(forecast_summary_file)\n",
    "    # Parse string lists into actual lists\n",
    "    for col in ['Predicted_Prices', 'Actual_Prices', 'Future_Price_Predictions', 'Train_Prices']:\n",
    "        if col in forecast_summary_df.columns:\n",
    "            forecast_summary_df[col] = forecast_summary_df[col].apply(\n",
    "                lambda x: [float(item.strip()) for item in x.split(',')] if pd.notnull(x) else []\n",
    "            )\n",
    "    logging.info(\"Loaded 'forecast_summary_XGBoost.csv' successfully.\")\n",
    "else:\n",
    "    forecast_summary_df = pd.DataFrame()\n",
    "    logging.warning(\"Could not find 'forecast_summary_XGBoost.csv'. Evaluation metrics and forecasts will not be displayed.\")\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server  # For deployment\n",
    "\n",
    "# Define app layout\n",
    "app.layout = html.Div([\n",
    "    html.H1('Stock Price Predictions Dashboard (XGBoost)'),\n",
    "    html.Div([\n",
    "        html.Label('Select Stock Symbol'),\n",
    "        dcc.Dropdown(\n",
    "            id='stock-dropdown',\n",
    "            options=[{'label': s, 'value': s} for s in symbols],\n",
    "            value='NVDA'  # Default stock\n",
    "        )\n",
    "    ], style={'width': '25%', 'display': 'inline-block'}),\n",
    "    html.Div([\n",
    "        html.Label('Select Date Range'),\n",
    "        dcc.DatePickerRange(\n",
    "            id='date-picker',\n",
    "            min_date_allowed='2014-09-18',  # Adjusted min date for 10-year period\n",
    "            max_date_allowed='2026-12-31',  # Extended max date to accommodate future forecasts\n",
    "            start_date='2014-09-18',        # Default start date\n",
    "            end_date='2025-09-18'           # Extended default end date to include 1-year forecast\n",
    "        )\n",
    "    ], style={'display': 'inline-block', 'marginLeft': '50px'}),\n",
    "    html.Div([\n",
    "        html.Label('Show 1-Year Forecast'),\n",
    "        dcc.Checklist(\n",
    "            id='forecast-checkbox',\n",
    "            options=[{'label': 'Include 1-Year Forecast', 'value': 'show_forecast'}],\n",
    "            value=[],  # Default unchecked\n",
    "            inline=True\n",
    "        )\n",
    "    ], style={'marginTop': '20px'}),\n",
    "    dcc.Graph(id='price-graph'),\n",
    "    html.Div(id='metrics-output', style={'marginTop': '20px'})\n",
    "])\n",
    "\n",
    "# Define callback to update graph and evaluation metrics\n",
    "@app.callback(\n",
    "    [Output('price-graph', 'figure'),\n",
    "     Output('metrics-output', 'children')],\n",
    "    [Input('stock-dropdown', 'value'),\n",
    "     Input('date-picker', 'start_date'),\n",
    "     Input('date-picker', 'end_date'),\n",
    "     Input('forecast-checkbox', 'value')]\n",
    ")\n",
    "def update_graph(selected_stock, start_date, end_date, forecast_option):\n",
    "    # Convert start_date and end_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Get the forecast data for the selected stock\n",
    "    forecast_row = forecast_summary_df[forecast_summary_df['Symbol'] == selected_stock]\n",
    "\n",
    "    if forecast_row.empty:\n",
    "        logging.error(f\"No forecast data found for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"No forecast data found for {selected_stock}.\"\n",
    "\n",
    "    # Parse 'Actual_Prices', 'Predicted_Prices', 'Future_Price_Predictions', 'Train_Prices'\n",
    "    actual_prices = forecast_row.iloc[0]['Actual_Prices']\n",
    "    predicted_prices = forecast_row.iloc[0]['Predicted_Prices']\n",
    "    future_prices = forecast_row.iloc[0]['Future_Price_Predictions']\n",
    "    train_prices = forecast_row.iloc[0]['Train_Prices']\n",
    "\n",
    "    # Load the stock data to get dates\n",
    "    stock_data_file = os.path.join(folder_path, f'{selected_stock}.csv')\n",
    "    if not os.path.exists(stock_data_file):\n",
    "        logging.error(f\"Stock data file not found for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Stock data file not found for {selected_stock}.\"\n",
    "\n",
    "    df_stock = pd.read_csv(stock_data_file)\n",
    "    if 'Date' not in df_stock.columns or 'Close' not in df_stock.columns:\n",
    "        logging.error(f\"Incorrect data format in stock data file for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Incorrect data format in stock data file for {selected_stock}.\"\n",
    "\n",
    "    # Convert 'Date' to datetime\n",
    "    df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
    "    df_stock_sorted = df_stock.sort_values('Date')\n",
    "    dates = df_stock_sorted['Date']\n",
    "\n",
    "    # Derive split_date based on 8-year training period\n",
    "    split_date = df_stock_sorted['Date'].min() + pd.DateOffset(years=8)\n",
    "\n",
    "    # Get dates for Train and Test based on split_date\n",
    "    train_mask = df_stock_sorted['Date'] <= split_date\n",
    "    test_mask = df_stock_sorted['Date'] > split_date\n",
    "\n",
    "    df_train = df_stock_sorted[train_mask]\n",
    "    df_test = df_stock_sorted[test_mask]\n",
    "\n",
    "    # Ensure that 'Actual_Prices' and 'Predicted_Prices' correspond to the test set\n",
    "    test_dates = df_test['Date'].reset_index(drop=True)\n",
    "    actual_prices = actual_prices[:len(test_dates)]  # Trim if necessary\n",
    "    predicted_prices = predicted_prices[:len(test_dates)]\n",
    "\n",
    "    if len(actual_prices) != len(predicted_prices):\n",
    "        logging.error(f\"Length mismatch between actual and predicted prices for {selected_stock}.\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': go.Layout(\n",
    "                title=f'{selected_stock} Price Prediction',\n",
    "                xaxis={'title': 'Date'},\n",
    "                yaxis={'title': 'Price'}\n",
    "            )\n",
    "        }, f\"Length mismatch in data for {selected_stock}.\"\n",
    "\n",
    "    # Create DataFrame for Train set within selected date range\n",
    "    df_train_filtered = df_train[(df_train['Date'] >= start_date) & (df_train['Date'] <= end_date)]\n",
    "\n",
    "    # Create DataFrame for Test set within selected date range\n",
    "    df_test_filtered = pd.DataFrame({\n",
    "        'Date': test_dates[:len(actual_prices)],\n",
    "        'Actual_Price': actual_prices,\n",
    "        'Predicted_Price': predicted_prices\n",
    "    })\n",
    "    df_test_filtered = df_test_filtered[\n",
    "        (df_test_filtered['Date'] >= start_date) & (df_test_filtered['Date'] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Initialize data list for plotting\n",
    "    data = []\n",
    "\n",
    "    # Add Train Prices if available in the selected date range\n",
    "    if not df_train_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_train_filtered['Date'],\n",
    "                y=df_train_filtered['Close'],\n",
    "                mode='lines',\n",
    "                name='Train Price',\n",
    "                line=dict(color='red')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add Actual Test Prices\n",
    "    if not df_test_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_test_filtered['Date'],\n",
    "                y=df_test_filtered['Actual_Price'],\n",
    "                mode='lines',\n",
    "                name='Actual Test Price',\n",
    "                line=dict(color='green')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add Predicted Test Prices\n",
    "    if not df_test_filtered.empty:\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=df_test_filtered['Date'],\n",
    "                y=df_test_filtered['Predicted_Price'],\n",
    "                mode='lines',\n",
    "                name='Predicted Test Price',\n",
    "                line=dict(color='blue')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add vertical line to indicate the split between train and test\n",
    "    if pd.notnull(split_date):\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=[split_date, split_date],\n",
    "                y=[df_stock_sorted['Close'].min(), df_stock_sorted['Close'].max()],\n",
    "                mode='lines',\n",
    "                name='Train-Test Split',\n",
    "                line=dict(color='black', dash='dash')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add future price predictions if selected\n",
    "    if 'show_forecast' in forecast_option:\n",
    "        # Generate future dates (252 business days)\n",
    "        last_date = df_stock_sorted['Date'].max()\n",
    "        future_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=len(future_prices))\n",
    "\n",
    "        if len(future_prices) != len(future_dates):\n",
    "            logging.error(f\"Length mismatch between future prices ({len(future_prices)}) and future dates ({len(future_dates)}) for {selected_stock}.\")\n",
    "        else:\n",
    "            df_future = pd.DataFrame({\n",
    "                'Date': future_dates,\n",
    "                'Forecasted_Price': future_prices\n",
    "            })\n",
    "\n",
    "            # Filter future forecasts based on selected date range\n",
    "            df_future_filtered = df_future[\n",
    "                (df_future['Date'] >= start_date) & (df_future['Date'] <= end_date)\n",
    "            ]\n",
    "\n",
    "            if not df_future_filtered.empty:\n",
    "                data.append(\n",
    "                    go.Scatter(\n",
    "                        x=df_future_filtered['Date'],\n",
    "                        y=df_future_filtered['Forecasted_Price'],\n",
    "                        mode='lines',\n",
    "                        name='1-Year Forecast',\n",
    "                        line=dict(dash='dash', color='orange')\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                logging.warning(f\"No forecasted data within the selected date range for {selected_stock}.\")\n",
    "\n",
    "    # Create figure with dynamic title based on selected date range\n",
    "    figure = {\n",
    "        'data': data,\n",
    "        'layout': go.Layout(\n",
    "            title=f'{selected_stock} Price Prediction ({start_date.date()} to {end_date.date()})',\n",
    "            xaxis={'title': 'Date'},\n",
    "            yaxis={'title': 'Price'},\n",
    "            hovermode='closest'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Get RMSE, MSE, and MAPE\n",
    "    rmse = forecast_row.iloc[0]['RMSE']\n",
    "    mse = forecast_row.iloc[0]['MSE']\n",
    "    mape = forecast_row.iloc[0]['MAPE']\n",
    "\n",
    "    # Format metrics output\n",
    "    if pd.notnull(rmse) and pd.notnull(mse) and pd.notnull(mape):\n",
    "        metrics_output = [\n",
    "            html.P(f'Root Mean Squared Error (RMSE): {rmse:.4f}'),\n",
    "            html.P(f'Mean Squared Error (MSE): {mse:.4f}'),\n",
    "            html.P(f'Mean Absolute Percentage Error (MAPE): {mape:.2%}')\n",
    "        ]\n",
    "    else:\n",
    "        metrics_output = [html.P(\"Evaluation metrics are not available.\")]\n",
    "\n",
    "    return figure, metrics_output\n",
    "\n",
    "# Run Dash app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4397fb0-57a2-48bd-8f06-9d9d46a29be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
